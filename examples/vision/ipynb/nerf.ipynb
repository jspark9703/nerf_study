{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jspark9703/nerf_study/blob/main/examples/vision/ipynb/nerf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73rs0uLCgINl"
      },
      "source": [
        "# 3D volumetric rendering with NeRF\n",
        "\n",
        "**Authors:** [Aritra Roy Gosthipaty](https://twitter.com/arig23498), [Ritwik Raha](https://twitter.com/ritwik_raha)<br>\n",
        "**Date created:** 2021/08/09<br>\n",
        "**Last modified:** 2023/11/13<br>\n",
        "**Description:** Minimal implementation of volumetric rendering as shown in NeRF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6snT5TZXgINo"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we present a minimal implementation of the research paper\n",
        "[**NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis**](https://arxiv.org/abs/2003.08934)\n",
        "by Ben Mildenhall et. al. The authors have proposed an ingenious way\n",
        "to *synthesize novel views of a scene* by modelling the *volumetric\n",
        "scene function* through a neural network.\n",
        "\n",
        "To help you understand this intuitively, let's start with the following question:\n",
        "*would it be possible to give to a neural\n",
        "network the position of a pixel in an image, and ask the network\n",
        "to predict the color at that position?*\n",
        "\n",
        "| ![2d-train](https://i.imgur.com/DQM92vN.png) |\n",
        "| :---: |\n",
        "| **Figure 1**: A neural network being given coordinates of an image\n",
        "as input and asked to predict the color at the coordinates. |\n",
        "\n",
        "The neural network would hypothetically *memorize* (overfit on) the\n",
        "image. This means that our neural network would have encoded the entire image\n",
        "in its weights. We could query the neural network with each position,\n",
        "and it would eventually reconstruct the entire image.\n",
        "\n",
        "| ![2d-test](https://i.imgur.com/6Qz5Hp1.png) |\n",
        "| :---: |\n",
        "| **Figure 2**: The trained neural network recreates the image from scratch. |\n",
        "\n",
        "A question now arises, how do we extend this idea to learn a 3D\n",
        "volumetric scene? Implementing a similar process as above would\n",
        "require the knowledge of every voxel (volume pixel). Turns out, this\n",
        "is quite a challenging task to do.\n",
        "\n",
        "The authors of the paper propose a minimal and elegant way to learn a\n",
        "3D scene using a few images of the scene. They discard the use of\n",
        "voxels for training. The network learns to model the volumetric scene,\n",
        "thus generating novel views (images) of the 3D scene that the model\n",
        "was not shown at training time.\n",
        "\n",
        "There are a few prerequisites one needs to understand to fully\n",
        "appreciate the process. We structure the example in such a way that\n",
        "you will have all the required knowledge before starting the\n",
        "implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3QCYArugINo"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "soYe8_fvgINp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "# Setting random seed to obtain reproducible results.\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio.v2 as imageio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize global variables.\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 5\n",
        "NUM_SAMPLES = 32\n",
        "POS_ENCODE_DIMS = 16\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J-VQhAwgINp"
      },
      "source": [
        "## Download and load the data\n",
        "\n",
        "The `npz` data file contains images, camera poses, and a focal length.\n",
        "The images are taken from multiple camera angles as shown in\n",
        "**Figure 3**.\n",
        "\n",
        "| ![camera-angles](https://i.imgur.com/FLsi2is.png) |\n",
        "| :---: |\n",
        "| **Figure 3**: Multiple camera angles <br>\n",
        "[Source: NeRF](https://arxiv.org/abs/2003.08934) |\n",
        "\n",
        "\n",
        "To understand camera poses in this context we have to first allow\n",
        "ourselves to think that a *camera is a mapping between the real-world\n",
        "and the 2-D image*.\n",
        "\n",
        "| ![mapping](https://www.mathworks.com/help/vision/ug/calibration_coordinate_blocks.png) |\n",
        "| :---: |\n",
        "| **Figure 4**: 3-D world to 2-D image mapping through a camera <br>\n",
        "[Source: Mathworks](https://www.mathworks.com/help/vision/ug/camera-calibration.html) |\n",
        "\n",
        "Consider the following equation:\n",
        "\n",
        "<img src=\"https://i.imgur.com/TQHKx5v.pngg\" width=\"100\" height=\"50\"/>\n",
        "\n",
        "Where **x** is the 2-D image point, **X** is the 3-D world point and\n",
        "**P** is the camera-matrix. **P** is a 3 x 4 matrix that plays the\n",
        "crucial role of mapping the real world object onto an image plane.\n",
        "\n",
        "<img src=\"https://i.imgur.com/chvJct5.png\" width=\"300\" height=\"100\"/>\n",
        "\n",
        "The camera-matrix is an *affine transform matrix* that is\n",
        "concatenated with a 3 x 1 column `[image height, image width, focal length]`\n",
        "to produce the *pose matrix*. This matrix is of\n",
        "dimensions 3 x 5 where the first 3 x 3 block is in the camera’s point\n",
        "of view. The axes are `[down, right, backwards]` or `[-y, x, z]`\n",
        "where the camera is facing forwards `-z`.\n",
        "\n",
        "| ![camera-mapping](https://i.imgur.com/kvjqbiO.png) |\n",
        "| :---: |\n",
        "| **Figure 5**: The affine transformation. |\n",
        "\n",
        "The COLMAP frame is `[right, down, forwards]` or `[x, -y, -z]`. Read\n",
        "more about COLMAP [here](https://colmap.github.io/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DHyvuaExgINq",
        "outputId": "df626ed4-6fd9-4505-8be7-33699208bbef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n",
            "\u001b[1m12727482/12727482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download the data if it does not already exist.\n",
        "url = (\n",
        "    \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
        ")\n",
        "data = keras.utils.get_file(origin=url)\n",
        "\n",
        "data = np.load(data)\n",
        "images = data[\"images\"]\n",
        "im_shape = images.shape\n",
        "(num_images, H, W, _) = images.shape\n",
        "(poses, focal) = (data[\"poses\"], data[\"focal\"])\n",
        "\n",
        "# Plot a random image from the dataset for visualization.\n",
        "plt.imshow(images[np.random.randint(low=0, high=num_images)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in data.files:\n",
        "    print(f\"- {key}: shape={data[key].shape}, dtype={data[key].dtype}\")"
      ],
      "metadata": {
        "id": "D7vTbFP27xfI",
        "outputId": "04f3a9dc-7a66-4a98-d4a1-a420e56fba04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- images: shape=(106, 100, 100, 3), dtype=float32\n",
            "- poses: shape=(106, 4, 4), dtype=float32\n",
            "- focal: shape=(), dtype=float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"예시 이미지 픽셀 값:\", images[0, 0, 0])  # 첫 이미지 왼쪽 위 픽셀\n",
        "print(\"예시 카메라 포즈:\\n\", poses[0])          # 첫 이미지의 카메라 포즈\n",
        "print(\"초점 거리 값:\", focal)                   # 초점 거리 값\n"
      ],
      "metadata": {
        "id": "GRk0rx358f3f",
        "outputId": "5b60497a-a09a-4487-da21-359e7b71c901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시 이미지 픽셀 값: [0. 0. 0.]\n",
            "예시 카메라 포즈:\n",
            " [[-9.9990219e-01  4.1922452e-03 -1.3345719e-02 -5.3798322e-02]\n",
            " [-1.3988681e-02 -2.9965907e-01  9.5394367e-01  3.8454704e+00]\n",
            " [-4.6566129e-10  9.5403719e-01  2.9968831e-01  1.2080823e+00]\n",
            " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]\n",
            "초점 거리 값: 138.88887889922103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg0aqIZ0gINq"
      },
      "source": [
        "## Data pipeline\n",
        "\n",
        "Now that you've understood the notion of camera matrix\n",
        "and the mapping from a 3D scene to 2D images,\n",
        "let's talk about the inverse mapping, i.e. from 2D image to the 3D scene.\n",
        "\n",
        "We'll need to talk about volumetric rendering with ray casting and tracing,\n",
        "which are common computer graphics techniques.\n",
        "This section will help you get to speed with these techniques.\n",
        "\n",
        "Consider an image with `N` pixels. We shoot a ray through each pixel\n",
        "and sample some points on the ray. A ray is commonly parameterized by\n",
        "the equation `r(t) = o + td` where `t` is the parameter, `o` is the\n",
        "origin and `d` is the unit directional vector as shown in **Figure 6**.\n",
        "\n",
        "| ![img](https://i.imgur.com/ywrqlzt.gif) |\n",
        "| :---: |\n",
        "| **Figure 6**: `r(t) = o + td` where t is 3 |\n",
        "\n",
        "In **Figure 7**, we consider a ray, and we sample some random points on\n",
        "the ray. These sample points each have a unique location `(x, y, z)`\n",
        "and the ray has a viewing angle `(theta, phi)`. The viewing angle is\n",
        "particularly interesting as we can shoot a ray through a single pixel\n",
        "in a lot of different ways, each with a unique viewing angle. Another\n",
        "interesting thing to notice here is the noise that is added to the\n",
        "sampling process. We add a uniform noise to each sample so that the\n",
        "samples correspond to a continuous distribution. In **Figure 7** the\n",
        "blue points are the evenly distributed samples and the white points\n",
        "`(t1, t2, t3)` are randomly placed between the samples.\n",
        "\n",
        "| ![img](https://i.imgur.com/r9TS2wv.gif) |\n",
        "| :---: |\n",
        "| **Figure 7**: Sampling the points from a ray. |\n",
        "\n",
        "**Figure 8** showcases the entire sampling process in 3D, where you\n",
        "can see the rays coming out of the white image. This means that each\n",
        "pixel will have its corresponding rays and each ray will be sampled at\n",
        "distinct points.\n",
        "\n",
        "| ![3-d rays](https://i.imgur.com/hr4D2g2.gif) |\n",
        "| :---: |\n",
        "| **Figure 8**: Shooting rays from all the pixels of an image in 3-D |\n",
        "\n",
        "These sampled points act as the input to the NeRF model. The model is\n",
        "then asked to predict the RGB color and the volume density at that\n",
        "point.\n",
        "\n",
        "| ![3-Drender](https://i.imgur.com/HHb6tlQ.png) |\n",
        "| :---: |\n",
        "| **Figure 9**: Data pipeline <br>\n",
        "[Source: NeRF](https://arxiv.org/abs/2003.08934) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rN0MJzTXgINr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_position(x):\n",
        "    \"\"\"Encodes the position into its corresponding Fourier feature.\n",
        "\n",
        "    Args:\n",
        "        x: The input coordinate.\n",
        "\n",
        "    Returns:\n",
        "        Fourier features tensors of the position.\n",
        "    \"\"\"\n",
        "    positions = [x]\n",
        "    for i in range(POS_ENCODE_DIMS):\n",
        "        for fn in [tf.sin, tf.cos]:\n",
        "            positions.append(fn(2.0**i * x))\n",
        "    return tf.concat(positions, axis=-1)\n",
        "\n",
        "\n",
        "def get_rays(height, width, focal, pose):\n",
        "    \"\"\"Computes origin point and direction vector of rays.\n",
        "\n",
        "    Args:\n",
        "        height: Height of the image.\n",
        "        width: Width of the image.\n",
        "        focal: The focal length between the images and the camera.\n",
        "        pose: The pose matrix of the camera.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of origin point and direction vector for rays.\n",
        "    \"\"\"\n",
        "    # Build a meshgrid for the rays.\n",
        "    i, j = tf.meshgrid(\n",
        "        tf.range(width, dtype=tf.float32),\n",
        "        tf.range(height, dtype=tf.float32),\n",
        "        indexing=\"xy\",\n",
        "    )\n",
        "\n",
        "    # Normalize the x axis coordinates.\n",
        "    transformed_i = (i - width * 0.5) / focal\n",
        "\n",
        "    # Normalize the y axis coordinates.\n",
        "    transformed_j = (j - height * 0.5) / focal\n",
        "\n",
        "    # Create the direction unit vectors.\n",
        "    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)\n",
        "\n",
        "    # Get the camera matrix.\n",
        "    camera_matrix = pose[:3, :3]\n",
        "    height_width_focal = pose[:3, -1]\n",
        "\n",
        "    # Get origins and directions for the rays.\n",
        "    transformed_dirs = directions[..., None, :]\n",
        "    camera_dirs = transformed_dirs * camera_matrix\n",
        "    ray_directions = tf.reduce_sum(camera_dirs, axis=-1)\n",
        "    ray_origins = tf.broadcast_to(height_width_focal, tf.shape(ray_directions))\n",
        "\n",
        "    # Return the origins and directions.\n",
        "    return (ray_origins, ray_directions)\n",
        "\n",
        "\n",
        "def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):\n",
        "    t_vals = tf.linspace(near, far, num_samples)\n",
        "    if rand:\n",
        "        shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
        "        noise = tf.random.uniform(shape=shape) * (far - near) / num_samples\n",
        "        t_vals = t_vals + noise\n",
        "\n",
        "    rays = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals[..., None]\n",
        "    rays_flat = tf.reshape(rays, [-1, 3])\n",
        "\n",
        "    # 각 샘플에 맞게 direction도 broadcast\n",
        "    directions = tf.broadcast_to(ray_directions[..., None, :], shape=rays.shape)\n",
        "    directions_flat = tf.reshape(directions, [-1, 3])\n",
        "\n",
        "    return rays_flat, directions_flat, t_vals\n",
        "\n",
        "\n",
        "def map_fn(pose):\n",
        "    ray_origins, ray_directions = get_rays(height=H, width=W, focal=focal, pose=pose)\n",
        "    rays_flat, dirs_flat, t_vals = render_flat_rays(\n",
        "        ray_origins=ray_origins,\n",
        "        ray_directions=ray_directions,\n",
        "        near=2.0,\n",
        "        far=6.0,\n",
        "        num_samples=NUM_SAMPLES,\n",
        "        rand=True,\n",
        "    )\n",
        "    return (ray_origins, ray_directions, t_vals)\n",
        "\n",
        "\n",
        "\n",
        "# Create the training split.\n",
        "split_index = int(num_images * 0.8)\n",
        "\n",
        "# Split the images into training and validation.\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "# Split the poses into training and validation.\n",
        "train_poses = poses[:split_index]\n",
        "val_poses = poses[split_index:]\n",
        "\n",
        "# Make the training pipeline.\n",
        "train_img_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_pose_ds = tf.data.Dataset.from_tensor_slices(train_poses)\n",
        "train_ray_ds = train_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
        "training_ds = tf.data.Dataset.zip((train_img_ds, train_ray_ds))\n",
        "train_ds = (\n",
        "    training_ds.shuffle(BATCH_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Make the validation pipeline.\n",
        "val_img_ds = tf.data.Dataset.from_tensor_slices(val_images)\n",
        "val_pose_ds = tf.data.Dataset.from_tensor_slices(val_poses)\n",
        "val_ray_ds = val_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
        "validation_ds = tf.data.Dataset.zip((val_img_ds, val_ray_ds))\n",
        "val_ds = (\n",
        "    validation_ds.shuffle(BATCH_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images_batch, (ray_origins, ray_directions, t_vals_batch) in train_ds.take(1):\n",
        "    print(\"✅ 이미지 배치\")\n",
        "    print(\"images_batch.shape:\", images_batch.shape)\n",
        "\n",
        "    print(\"\\n✅ 광선 원점 (ray origins)\")\n",
        "    print(\"ray_origins.shape:\", ray_origins.shape)\n",
        "\n",
        "    print(\"\\n✅ 광선 방향 (ray directions)\")\n",
        "    print(\"ray_directions.shape:\", ray_directions.shape)\n",
        "\n",
        "    print(\"\\n✅ 각 샘플 포인트의 깊이 값 (t_vals)\")\n",
        "    print(\"t_vals_batch.shape:\", t_vals_batch.shape)\n"
      ],
      "metadata": {
        "id": "abvJPILW9VFE",
        "outputId": "7f712f9e-b2e0-4ed3-f5a7-3adaeced28b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 이미지 배치\n",
            "images_batch.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 광선 원점 (ray origins)\n",
            "ray_origins.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 광선 방향 (ray directions)\n",
            "ray_directions.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 각 샘플 포인트의 깊이 값 (t_vals)\n",
            "t_vals_batch.shape: (5, 100, 100, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxrKm7PegINr"
      },
      "source": [
        "## NeRF model\n",
        "\n",
        "The model is a multi-layer perceptron (MLP), with ReLU as its non-linearity.\n",
        "\n",
        "An excerpt from the paper:\n",
        "\n",
        "*\"We encourage the representation to be multiview-consistent by\n",
        "restricting the network to predict the volume density sigma as a\n",
        "function of only the location `x`, while allowing the RGB color `c` to be\n",
        "predicted as a function of both location and viewing direction. To\n",
        "accomplish this, the MLP first processes the input 3D coordinate `x`\n",
        "with 8 fully-connected layers (using ReLU activations and 256 channels\n",
        "per layer), and outputs sigma and a 256-dimensional feature vector.\n",
        "This feature vector is then concatenated with the camera ray's viewing\n",
        "direction and passed to one additional fully-connected layer (using a\n",
        "ReLU activation and 128 channels) that output the view-dependent RGB\n",
        "color.\"*\n",
        "\n",
        "Here we have gone for a minimal implementation and have used 64\n",
        "Dense units instead of 256 as mentioned in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A4EzZHbdgINs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_nerf_model(num_layers=8, num_pos=None):\n",
        "    \"\"\"Accurate NeRF MLP with residual connection and full separation between σ and RGB heads.\"\"\"\n",
        "    pos_input = keras.Input(shape=(2 * 3 * POS_ENCODE_DIMS + 3))  # encoded (x, y, z)\n",
        "    dir_input = keras.Input(shape=(2 * 3 * POS_ENCODE_DIMS + 3))  # encoded viewing dir\n",
        "\n",
        "    # ----------- Position encoding pathway (8 layers + skip) -----------\n",
        "    x = pos_input\n",
        "    for i in range(8):\n",
        "        x = layers.Dense(256, activation=\"relu\")(x)\n",
        "        if i == 4:\n",
        "            x = layers.concatenate([x, pos_input])\n",
        "\n",
        "    # ----------- Sigma (Density) head -----------\n",
        "    sigma = layers.Dense(1)(x)  # No activation here — will be relu’d later\n",
        "\n",
        "    # ----------- Feature vector to RGB path -----------\n",
        "    feature = layers.Dense(256, activation=\"relu\")(x)  # bottleneck feature\n",
        "    h = layers.concatenate([feature, dir_input])  # concat with encoded dir\n",
        "    h = layers.Dense(128, activation=\"relu\")(h)\n",
        "    rgb = layers.Dense(3, activation=\"sigmoid\")(h)  # sigmoid for [0,1] RGB\n",
        "\n",
        "    # ----------- Final output -----------\n",
        "    output = layers.concatenate([rgb, sigma], axis=-1)  # shape: (batch, 4)\n",
        "    return keras.Model(inputs=[pos_input, dir_input], outputs=output)\n",
        "\n",
        "def render_rgb_depth(model, rays_flat, directions_flat, t_vals, rand=True, train=True):\n",
        "    encoded_pos = encode_position(rays_flat)\n",
        "    encoded_dir = encode_position(directions_flat)\n",
        "\n",
        "    model_input = [encoded_pos, encoded_dir]\n",
        "\n",
        "    if train:\n",
        "        predictions = model(model_input)\n",
        "    else:\n",
        "        predictions = model.predict(model_input)\n",
        "\n",
        "    predictions = tf.reshape(predictions, shape=(BATCH_SIZE, H, W, NUM_SAMPLES, 4))\n",
        "    rgb = tf.sigmoid(predictions[..., :-1])\n",
        "    sigma_a = tf.nn.relu(predictions[..., -1])\n",
        "\n",
        "    delta = t_vals[..., 1:] - t_vals[..., :-1]\n",
        "    delta = tf.concat(\n",
        "        [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, H, W, 1))], axis=-1\n",
        "    )\n",
        "\n",
        "    alpha = 1.0 - tf.exp(-sigma_a * delta)\n",
        "    transmittance = tf.math.cumprod(1.0 - alpha + 1e-10, axis=-1, exclusive=True)\n",
        "    weights = alpha * transmittance\n",
        "\n",
        "    rgb_map = tf.reduce_sum(weights[..., None] * rgb, axis=-2)\n",
        "    depth_map = tf.reduce_sum(weights * t_vals, axis=-1)\n",
        "    return rgb_map, depth_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giPXy26MgINs"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training step is implemented as part of a custom `keras.Model` subclass\n",
        "so that we can make use of the `model.fit` functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nZ4FZ9NPgINs",
        "outputId": "a81c9bf3-5c34-410e-ca2d-4fd8ea0bf895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '99' to a shape.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d37cddb72a42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mcoarse_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mfine_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-18c4533c1101>\u001b[0m in \u001b[0;36mget_nerf_model\u001b[0;34m(num_layers, num_pos)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Accurate NeRF MLP with residual connection and full separation between σ and RGB heads.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPOS_ENCODE_DIMS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoded (x, y, z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdir_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPOS_ENCODE_DIMS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoded viewing dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     layer = InputLayer(\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Undefined shapes are not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot convert '{shape}' to a shape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tensorflow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '99' to a shape."
          ]
        }
      ],
      "source": [
        "\n",
        "class NeRF(keras.Model):\n",
        "    def __init__(self, coarse_model, fine_model, num_fine):\n",
        "        super().__init__()\n",
        "        self.coarse_model = coarse_model\n",
        "        self.fine_model = fine_model\n",
        "        self.num_fine = num_fine\n",
        "\n",
        "    def compile(self, optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.psnr_tracker = keras.metrics.Mean(name=\"psnr\")\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        images, rays = inputs\n",
        "        rays_o, rays_d, t_vals = rays  # outputs of render_flat_rays()\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            rgb_coarse, rgb_fine = render_nerf_dual(\n",
        "                coarse_model=self.coarse_model,\n",
        "                fine_model=self.fine_model,\n",
        "                rays_o=rays_o,\n",
        "                rays_d=rays_d,\n",
        "                t_vals=t_vals,\n",
        "                num_fine=self.num_fine,\n",
        "                rand=True\n",
        "            )\n",
        "\n",
        "            # Compute losses\n",
        "            loss_coarse = self.loss_fn(images, rgb_coarse)\n",
        "            loss_fine = self.loss_fn(images, rgb_fine)\n",
        "            loss = loss_coarse + loss_fine\n",
        "\n",
        "        variables = self.coarse_model.trainable_variables + self.fine_model.trainable_variables\n",
        "        grads = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, variables))\n",
        "\n",
        "        psnr = tf.image.psnr(images, rgb_fine, max_val=1.0)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.psnr_tracker.update_state(psnr)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_tracker.result()}\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        images, rays = inputs\n",
        "        rays_o, rays_d, t_vals = rays\n",
        "        test_rgb_c, test_rgb_f = render_nerf_dual(\n",
        "            coarse_model=coarse_model,\n",
        "            fine_model=fine_model,\n",
        "            rays_o=rays_o,\n",
        "            rays_d=rays_d,\n",
        "            t_vals=test_t_vals,\n",
        "            num_fine=128,\n",
        "            rand=False\n",
        "        )\n",
        "        loss_coarse = self.loss_fn(images, test_rgb_c)\n",
        "        loss_fine = self.loss_fn(images, test_rgb_f)\n",
        "        loss = loss_coarse + loss_fine\n",
        "        psnr = tf.image.psnr(images, test_rgb_f, max_val=1.0)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.psnr_tracker.update_state(psnr)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.psnr_tracker]\n",
        "\n",
        "test_imgs, test_rays = next(iter(train_ds))\n",
        "rays_o, rays_d, test_t_vals = test_rays\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "\n",
        "class TrainMonitor(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        loss = logs[\"loss\"]\n",
        "        loss_list.append(loss)\n",
        "        test_recons_images, depth_maps = render_rgb_depth(\n",
        "            model=self.model.nerf_model,\n",
        "            rays_flat=test_rays_flat,\n",
        "            t_vals=test_t_vals,\n",
        "            rand=True,\n",
        "            train=False,\n",
        "        )\n",
        "\n",
        "        # Plot the rgb, depth and the loss plot.\n",
        "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
        "        ax[0].imshow(keras.utils.array_to_img(test_recons_images[0]))\n",
        "        ax[0].set_title(f\"Predicted Image: {epoch:03d}\")\n",
        "\n",
        "        ax[1].imshow(keras.utils.array_to_img(depth_maps[0, ..., None]))\n",
        "        ax[1].set_title(f\"Depth Map: {epoch:03d}\")\n",
        "\n",
        "        ax[2].plot(loss_list)\n",
        "        ax[2].set_xticks(np.arange(0, EPOCHS + 1, 5.0))\n",
        "        ax[2].set_title(f\"Loss Plot: {epoch:03d}\")\n",
        "\n",
        "        fig.savefig(f\"images/{epoch:03d}.png\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "num_pos = H * W * NUM_SAMPLES\n",
        "coarse_model =  get_nerf_model(num_layers=8, num_pos=num_pos)\n",
        "fine_model =  get_nerf_model(num_layers=8, num_pos=num_pos)\n",
        "\n",
        "def sample_pdf(bins, weights, num_fine_samples, det=False):\n",
        "    weights += 1e-5  # prevent nans\n",
        "    pdf = weights / tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
        "    cdf = tf.cumsum(pdf, axis=-1)\n",
        "    cdf = tf.concat([tf.zeros_like(cdf[..., :1]), cdf], axis=-1)\n",
        "\n",
        "    if det:\n",
        "        u = tf.linspace(0., 1., num_fine_samples)\n",
        "        u = tf.broadcast_to(u, tf.shape(cdf[..., 1:]))\n",
        "    else:\n",
        "        u = tf.random.uniform(tf.shape(cdf[..., 1:]), dtype=pdf.dtype)\n",
        "\n",
        "    u = tf.sort(u, axis=-1)\n",
        "\n",
        "    inds = tf.searchsorted(cdf, u, side='right')\n",
        "    below = tf.maximum(0, inds - 1)\n",
        "    above = tf.minimum(cdf.shape[-1] - 1, inds)\n",
        "\n",
        "    inds_sampled = tf.stack([below, above], axis=-1)\n",
        "    matched_shape = [inds_sampled.shape[0], inds_sampled.shape[1], cdf.shape[-1]]\n",
        "    cdf_g = tf.gather(cdf, inds_sampled, axis=-1, batch_dims=1)\n",
        "    bins_g = tf.gather(bins, inds_sampled, axis=-1, batch_dims=1)\n",
        "\n",
        "    denom = cdf_g[..., 1] - cdf_g[..., 0]\n",
        "    denom = tf.where(denom < 1e-5, tf.ones_like(denom), denom)\n",
        "    t = (u - cdf_g[..., 0]) / denom\n",
        "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
        "    return samples\n",
        "\n",
        "def hierarchical_sample(ray_origins, ray_directions, t_vals_coarse, weights_coarse, num_fine):\n",
        "    mid = 0.5 * (t_vals_coarse[..., :-1] + t_vals_coarse[..., 1:])\n",
        "    fine_t_vals = sample_pdf(mid, weights_coarse[..., 1:-1], num_fine)\n",
        "\n",
        "    t_vals_all = tf.sort(tf.concat([t_vals_coarse, fine_t_vals], axis=-1), axis=-1)\n",
        "\n",
        "    # r(t) = o + td\n",
        "    sample_points = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals_all[..., None]\n",
        "    directions = tf.broadcast_to(ray_directions[..., None, :], sample_points.shape)\n",
        "    points_flat = tf.reshape(sample_points, [-1, 3])\n",
        "    dirs_flat = tf.reshape(directions, [-1, 3])\n",
        "\n",
        "    return points_flat, dirs_flat, t_vals_all\n",
        "\n",
        "def render_nerf_dual(coarse_model, fine_model, rays_o, rays_d, t_vals, num_fine, rand=True):\n",
        "    # 1. Coarse\n",
        "    pts = rays_o[..., None, :] + rays_d[..., None, :] * t_vals[..., None]\n",
        "    dirs = tf.broadcast_to(rays_d[..., None, :], pts.shape)\n",
        "    pts_flat = tf.reshape(pts, [-1, 3])\n",
        "    dirs_flat = tf.reshape(dirs, [-1, 3])\n",
        "\n",
        "    encoded_pts = encode_position(pts_flat)\n",
        "    encoded_dirs = encode_position(dirs_flat)\n",
        "    preds = coarse_model([encoded_pts, encoded_dirs])\n",
        "    preds = tf.reshape(preds, (*t_vals.shape, 4))\n",
        "    rgb_c, sigma_c = preds[..., :-1], tf.nn.relu(preds[..., -1])\n",
        "\n",
        "    # 2. Rendering coarse\n",
        "    delta = t_vals[..., 1:] - t_vals[..., :-1]\n",
        "    delta = tf.concat([delta, tf.broadcast_to([1e10], tf.shape(delta[..., :1]))], axis=-1)\n",
        "    alpha = 1.0 - tf.exp(-sigma_c * delta)\n",
        "    T = tf.math.cumprod(1.0 - alpha + 1e-10, axis=-1, exclusive=True)\n",
        "    weights = alpha * T\n",
        "    rgb_map_coarse = tf.reduce_sum(weights[..., None] * tf.sigmoid(rgb_c), axis=-2)\n",
        "\n",
        "    # 3. Fine sampling\n",
        "    pts_fine_flat, dirs_fine_flat, t_vals_fine = hierarchical_sample(\n",
        "        rays_o, rays_d, t_vals, weights, num_fine\n",
        "    )\n",
        "    encoded_pts_f = encode_position(pts_fine_flat)\n",
        "    encoded_dirs_f = encode_position(dirs_fine_flat)\n",
        "    preds_f = fine_model([encoded_pts_f, encoded_dirs_f])\n",
        "    preds_f = tf.reshape(preds_f, (*t_vals_fine.shape, 4))\n",
        "    rgb_f, sigma_f = preds_f[..., :-1], tf.nn.relu(preds_f[..., -1])\n",
        "\n",
        "    # 4. Rendering fine\n",
        "    delta_f = t_vals_fine[..., 1:] - t_vals_fine[..., :-1]\n",
        "    delta_f = tf.concat([delta_f, tf.broadcast_to([1e10], tf.shape(delta_f[..., :1]))], axis=-1)\n",
        "    alpha_f = 1.0 - tf.exp(-sigma_f * delta_f)\n",
        "    T_f = tf.math.cumprod(1.0 - alpha_f + 1e-10, axis=-1, exclusive=True)\n",
        "    weights_f = alpha_f * T_f\n",
        "    rgb_map_fine = tf.reduce_sum(weights_f[..., None] * tf.sigmoid(rgb_f), axis=-2)\n",
        "\n",
        "    return rgb_map_coarse, rgb_map_fine\n",
        "\n",
        "model = NeRF(coarse_model, fine_model, num_fine=128)\n",
        "model.compile(\n",
        "   optimizer=keras.optimizers.Adam(5e-4),\n",
        "    loss_fn=keras.losses.MeanSquaredError())\n",
        "\n",
        "# Create a directory to save the images during training.\n",
        "if not os.path.exists(\"images\"):\n",
        "    os.makedirs(\"images\")\n",
        "\n",
        "\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[TrainMonitor()])\n",
        "\n",
        "def create_gif(path_to_images, name_gif):\n",
        "    filenames = glob.glob(path_to_images)\n",
        "    filenames = sorted(filenames)\n",
        "    images = []\n",
        "    for filename in tqdm(filenames):\n",
        "        images.append(imageio.imread(filename))\n",
        "    kargs = {\"duration\": 0.25}\n",
        "    imageio.mimsave(name_gif, images, \"GIF\", **kargs)\n",
        "\n",
        "\n",
        "create_gif(\"images/*.png\", \"training.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-OdxcdlgINs"
      },
      "source": [
        "## Visualize the training step\n",
        "\n",
        "Here we see the training step. With the decreasing loss, the rendered\n",
        "image and the depth maps are getting better. In your local system, you\n",
        "will see the `training.gif` file generated.\n",
        "\n",
        "![training-20](https://i.imgur.com/ql5OcYA.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMYL_1-DgINt"
      },
      "source": [
        "## Inference\n",
        "\n",
        "In this section, we ask the model to build novel views of the scene.\n",
        "The model was given `106` views of the scene in the training step. The\n",
        "collections of training images cannot contain each and every angle of\n",
        "the scene. A trained model can represent the entire 3-D scene with a\n",
        "sparse set of training images.\n",
        "\n",
        "Here we provide different poses to the model and ask for it to give us\n",
        "the 2-D image corresponding to that camera view. If we infer the model\n",
        "for all the 360-degree views, it should provide an overview of the\n",
        "entire scenery from all around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "put9-kICgINt"
      },
      "outputs": [],
      "source": [
        "# Get the trained NeRF model and infer.\n",
        "nerf_model = model.nerf_model\n",
        "test_recons_images, depth_maps = render_rgb_depth(\n",
        "    model=self.model,\n",
        "    rays_flat=test_rays_flat,\n",
        "    t_vals=test_t_vals,\n",
        "    rand=True,\n",
        "    train=False,\n",
        ")\n",
        "# Create subplots.\n",
        "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(10, 20))\n",
        "\n",
        "for ax, ori_img, recons_img, depth_map in zip(\n",
        "    axes, test_imgs, test_recons_images, depth_maps\n",
        "):\n",
        "    ax[0].imshow(keras.utils.array_to_img(ori_img))\n",
        "    ax[0].set_title(\"Original\")\n",
        "\n",
        "    ax[1].imshow(keras.utils.array_to_img(recons_img))\n",
        "    ax[1].set_title(\"Reconstructed\")\n",
        "\n",
        "    ax[2].imshow(keras.utils.array_to_img(depth_map[..., None]), cmap=\"inferno\")\n",
        "    ax[2].set_title(\"Depth Map\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHxKnNv3gINt"
      },
      "source": [
        "## Render 3D Scene\n",
        "\n",
        "Here we will synthesize novel 3D views and stitch all of them together\n",
        "to render a video encompassing the 360-degree view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAxcjwS-gINt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_translation_t(t):\n",
        "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "    \"\"\"\n",
        "    Get the camera to world matrix for the corresponding theta, phi\n",
        "    and t.\n",
        "    \"\"\"\n",
        "    c2w = get_translation_t(t)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "rgb_frames = []\n",
        "batch_flat = []\n",
        "batch_t = []\n",
        "\n",
        "# Iterate over different theta value and generate scenes.\n",
        "for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 120, endpoint=False))):\n",
        "    # Get the camera to world matrix.\n",
        "    c2w = pose_spherical(theta, -30.0, 4.0)\n",
        "\n",
        "    #\n",
        "    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)\n",
        "    rays_flat, t_vals = render_flat_rays(\n",
        "        ray_oris, ray_dirs, near=2.0, far=6.0, num_samples=NUM_SAMPLES, rand=False\n",
        "    )\n",
        "\n",
        "    if index % BATCH_SIZE == 0 and index > 0:\n",
        "        batched_flat = tf.stack(batch_flat, axis=0)\n",
        "        batch_flat = [rays_flat]\n",
        "\n",
        "        batched_t = tf.stack(batch_t, axis=0)\n",
        "        batch_t = [t_vals]\n",
        "\n",
        "        rgb, _ = render_rgb_depth(\n",
        "            nerf_model, batched_flat, batched_t, rand=False, train=False\n",
        "        )\n",
        "\n",
        "        temp_rgb = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in rgb]\n",
        "\n",
        "        rgb_frames = rgb_frames + temp_rgb\n",
        "    else:\n",
        "        batch_flat.append(rays_flat)\n",
        "        batch_t.append(t_vals)\n",
        "\n",
        "rgb_video = \"rgb_video.mp4\"\n",
        "imageio.mimwrite(rgb_video, rgb_frames, fps=30, quality=7, macro_block_size=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402qKS5pgINt"
      },
      "source": [
        "### Visualize the video\n",
        "\n",
        "Here we can see the rendered 360 degree view of the scene. The model\n",
        "has successfully learned the entire volumetric space through the\n",
        "sparse set of images in **only 20 epochs**. You can view the\n",
        "rendered video saved locally, named `rgb_video.mp4`.\n",
        "\n",
        "![rendered-video](https://i.imgur.com/j2sIkzW.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wc37TAFgINt"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We have produced a minimal implementation of NeRF to provide an intuition of its\n",
        "core ideas and methodology. This method has been used in various\n",
        "other works in the computer graphics space.\n",
        "\n",
        "We would like to encourage our readers to use this code as an example\n",
        "and play with the hyperparameters and visualize the outputs. Below we\n",
        "have also provided the outputs of the model trained for more epochs.\n",
        "\n",
        "| Epochs | GIF of the training step |\n",
        "| :--- | :---: |\n",
        "| **100** | ![100-epoch-training](https://i.imgur.com/2k9p8ez.gif) |\n",
        "| **200** | ![200-epoch-training](https://i.imgur.com/l3rG4HQ.gif) |\n",
        "\n",
        "## Way forward\n",
        "\n",
        "If anyone is interested to go deeper into NeRF, we have built a 3-part blog\n",
        "series at [PyImageSearch](https://pyimagesearch.com/).\n",
        "\n",
        "- [Prerequisites of NeRF](https://www.pyimagesearch.com/2021/11/10/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-1/)\n",
        "- [Concepts of NeRF](https://www.pyimagesearch.com/2021/11/17/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-2/)\n",
        "- [Implementing NeRF](https://www.pyimagesearch.com/2021/11/24/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-3/)\n",
        "\n",
        "## Reference\n",
        "\n",
        "- [NeRF repository](https://github.com/bmild/nerf): The official\n",
        "    repository for NeRF.\n",
        "- [NeRF paper](https://arxiv.org/abs/2003.08934): The paper on NeRF.\n",
        "- [Manim Repository](https://github.com/3b1b/manim): We have used\n",
        "    manim to build all the animations.\n",
        "- [Mathworks](https://www.mathworks.com/help/vision/ug/camera-calibration.html):\n",
        "    Mathworks for the camera calibration article.\n",
        "- [Mathew's video](https://www.youtube.com/watch?v=dPWLybp4LL0): A\n",
        "    great video on NeRF.\n",
        "\n",
        "You can try the model on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/NeRF)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nerf",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}