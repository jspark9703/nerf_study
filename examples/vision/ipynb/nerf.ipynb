{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jspark9703/nerf_study/blob/main/examples/vision/ipynb/nerf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3QCYArugINo"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rN0MJzTXgINr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Initialize global variables (가정)\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 5\n",
        "NUM_SAMPLES_COARSE = 64 # 예시 값\n",
        "H = 100 # 예시 값, 실제 데이터 로드 후 설정 필요\n",
        "W = 100 # 예시 값, 실제 데이터 로드 후 설정 필요\n",
        "focal = 138.8889 # 예시 값, 실제 데이터 로드 후 설정 필요\n",
        "\n",
        "# --- 데이터 로드 부분 (사용자 코드에서 가져옴) ---\n",
        "url = (\n",
        "    \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
        ")\n",
        "data_path = keras.utils.get_file(origin=url)\n",
        "data = np.load(data_path)\n",
        "images = data[\"images\"]\n",
        "(num_images_total, H_data, W_data, _) = images.shape # H, W, focal을 여기서 업데이트\n",
        "poses = data[\"poses\"]\n",
        "focal_data = data[\"focal\"]\n",
        "\n",
        "# 전역 변수 업데이트\n",
        "H = H_data\n",
        "W = W_data\n",
        "focal = focal_data.astype(np.float32) # focal을 float32로 변환\n",
        "\n",
        "# -------------------------------------------\n",
        "\n",
        "def encode_position(x, num_encoding_functions):\n",
        "    \"\"\"Encodes the position or direction into its corresponding Fourier feature.\n",
        "\n",
        "    Args:\n",
        "        x: The input coordinate or direction (3D).\n",
        "        num_encoding_functions: The L value for positional encoding.\n",
        "\n",
        "    Returns:\n",
        "        Fourier features tensors of the position or direction.\n",
        "    \"\"\"\n",
        "    positions = [x]\n",
        "    for i in range(num_encoding_functions):\n",
        "        for fn in [tf.sin, tf.cos]:\n",
        "            positions.append(fn((2.0**i) * x))\n",
        "    return tf.concat(positions, axis=-1)\n",
        "\n",
        "\n",
        "def get_rays(height, width, focal_val, pose): # focal 인자 이름 변경 (전역 focal과 구분)\n",
        "    \"\"\"Computes origin point and direction vector of rays.\n",
        "\n",
        "    Args:\n",
        "        height: Height of the image.\n",
        "        width: Width of the image.\n",
        "        focal_val: The focal length between the images and the camera.\n",
        "        pose: The pose matrix of the camera (camera-to-world).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of origin point and direction vector for rays.\n",
        "    \"\"\"\n",
        "    i, j = tf.meshgrid(\n",
        "        tf.range(width, dtype=tf.float32),\n",
        "        tf.range(height, dtype=tf.float32),\n",
        "        indexing=\"xy\",\n",
        "    )\n",
        "    transformed_i = (i - width * 0.5) / focal_val\n",
        "    transformed_j = (j - height * 0.5) / focal_val\n",
        "    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)\n",
        "    ray_directions = tf.reduce_sum(directions[..., None, :] * pose[:3, :3], axis=-1)\n",
        "    ray_origins = tf.broadcast_to(pose[:3, -1], tf.shape(ray_directions))\n",
        "    return (ray_origins, ray_directions)\n",
        "\n",
        "\n",
        "def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):\n",
        "    \"\"\"Generates sample points along rays.\n",
        "\n",
        "    Args:\n",
        "        ray_origins: Origin of the rays (H, W, 3) when called from map_fn.\n",
        "        ray_directions: Direction of the rays (H, W, 3) when called from map_fn.\n",
        "        near: The near bound for sampling.\n",
        "        far: The far bound for sampling.\n",
        "        num_samples: Number of samples per ray.\n",
        "        rand: If True, add random noise to sampling positions.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of:\n",
        "            rays_flat: Sampled points (H * W * num_samples, 3).\n",
        "            directions_flat: Ray directions corresponding to each sample point\n",
        "                             (H * W * num_samples, 3).\n",
        "            t_vals: The t values for sampling along each ray (H, W, num_samples).\n",
        "    \"\"\"\n",
        "    t_vals_linspace = tf.linspace(near, far, num_samples)\n",
        "    current_shape_prefix = tf.shape(ray_origins)[:-1] # (H, W)\n",
        "\n",
        "    if rand:\n",
        "        noise_shape = tf.concat([current_shape_prefix, [num_samples]], axis=0) # (H, W, num_samples)\n",
        "        noise = tf.random.uniform(shape=noise_shape) * (far - near) / num_samples\n",
        "        t_vals = tf.broadcast_to(t_vals_linspace, noise_shape) + noise\n",
        "    else:\n",
        "        t_vals_shape = tf.concat([current_shape_prefix, [num_samples]], axis=0) # (H, W, num_samples)\n",
        "        t_vals = tf.broadcast_to(t_vals_linspace, t_vals_shape)\n",
        "\n",
        "    rays = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals[..., None]\n",
        "    rays_flat = tf.reshape(rays, [-1, 3])\n",
        "\n",
        "    directions = tf.broadcast_to(ray_directions[..., None, :], tf.shape(rays))\n",
        "    directions_flat = tf.reshape(directions, [-1, 3])\n",
        "\n",
        "    return rays_flat, directions_flat, t_vals\n",
        "\n",
        "# map_fn_coarse는 (pose, image) 쌍을 처리합니다.\n",
        "def map_fn_coarse(pose, image): # 두 개의 인자로 명시적으로 받음\n",
        "    # 전역 H, W, focal 사용\n",
        "    ray_origins, ray_directions = get_rays(height=H, width=W, focal_val=focal, pose=pose)\n",
        "\n",
        "    rays_flat, dirs_flat, t_vals = render_flat_rays(\n",
        "        ray_origins=ray_origins,\n",
        "        ray_directions=ray_directions,\n",
        "        near=2.0,\n",
        "        far=6.0,\n",
        "        num_samples=NUM_SAMPLES_COARSE,\n",
        "        rand=True,\n",
        "    )\n",
        "    # t_vals의 shape은 (H, W, NUM_SAMPLES_COARSE)가 됩니다.\n",
        "    return (image, (ray_origins, ray_directions, t_vals))\n",
        "\n",
        "# Create the training split.\n",
        "split_index = int(num_images_total * 0.8)\n",
        "\n",
        "# Split the images into training and validation.\n",
        "train_images_data = images[:split_index]\n",
        "val_images_data = images[split_index:]\n",
        "\n",
        "# Split the poses into training and validation.\n",
        "train_poses_data = poses[:split_index]\n",
        "val_poses_data = poses[split_index:]\n",
        "\n",
        "# Make the training pipeline.\n",
        "train_img_ds = tf.data.Dataset.from_tensor_slices(train_images_data.astype(np.float32)/255.0)\n",
        "train_pose_ds = tf.data.Dataset.from_tensor_slices(train_poses_data.astype(np.float32))\n",
        "# train_dataset_raw의 각 요소는 (pose_tensor, image_tensor) 튜플입니다.\n",
        "train_dataset_raw = tf.data.Dataset.zip((train_pose_ds, train_img_ds))\n",
        "\n",
        "# Map poses to rays and t_vals for coarse sampling\n",
        "# map 함수는 train_dataset_raw의 각 요소를 풀어서 map_fn_coarse에 전달합니다.\n",
        "# 즉, map_fn_coarse(pose_tensor, image_tensor) 형태로 호출됩니다.\n",
        "train_ds = (\n",
        "    train_dataset_raw.map(map_fn_coarse, num_parallel_calls=AUTO)\n",
        "    .shuffle(BATCH_SIZE * 10)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "\n",
        "# Make the validation pipeline.\n",
        "val_img_ds = tf.data.Dataset.from_tensor_slices(val_images_data.astype(np.float32)/255.0)\n",
        "val_pose_ds = tf.data.Dataset.from_tensor_slices(val_poses_data.astype(np.float32))\n",
        "val_dataset_raw = tf.data.Dataset.zip((val_pose_ds, val_img_ds))\n",
        "\n",
        "val_ds = (\n",
        "    val_dataset_raw.map(map_fn_coarse, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# 실행 테스트 (데이터셋에서 첫 번째 배치 가져오기)\n",
        "# for images_batch, (rays_o_batch, rays_d_batch, t_vals_coarse_batch) in train_ds.take(1):\n",
        "#     print(\"Images batch shape:\", images_batch.shape)\n",
        "#     print(\"Ray origins batch shape:\", rays_o_batch.shape)\n",
        "#     print(\"Ray directions batch shape:\", rays_d_batch.shape)\n",
        "#     print(\"Coarse t_vals batch shape:\", t_vals_coarse_batch.shape)\n",
        "#     # t_vals_coarse_batch는 (BATCH_SIZE, H, W, NUM_SAMPLES_COARSE)가 되어야 합니다.\n",
        "#     break\n",
        "\n",
        "# print(\"Dataset pipeline created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images_batch, (ray_origins, ray_directions, t_vals_batch) in train_ds.take(1):\n",
        "    print(\"✅ 이미지 배치\")\n",
        "    print(\"images_batch.shape:\", images_batch.shape)\n",
        "\n",
        "    print(\"\\n✅ 광선 원점 (ray origins)\")\n",
        "    print(\"ray_origins.shape:\", ray_origins.shape)\n",
        "\n",
        "    print(\"\\n✅ 광선 방향 (ray directions)\")\n",
        "    print(\"ray_directions.shape:\", ray_directions.shape)\n",
        "\n",
        "    print(\"\\n✅ 각 샘플 포인트의 깊이 값 (t_vals)\")\n",
        "    print(\"t_vals_batch.shape:\", t_vals_batch.shape)\n"
      ],
      "metadata": {
        "id": "abvJPILW9VFE",
        "outputId": "1018ce98-0de6-41e2-c3bf-0fe5d853f763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 이미지 배치\n",
            "images_batch.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 광선 원점 (ray origins)\n",
            "ray_origins.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 광선 방향 (ray directions)\n",
            "ray_directions.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 각 샘플 포인트의 깊이 값 (t_vals)\n",
            "t_vals_batch.shape: (5, 100, 100, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxrKm7PegINr"
      },
      "source": [
        "## NeRF model\n",
        "\n",
        "The model is a multi-layer perceptron (MLP), with ReLU as its non-linearity.\n",
        "\n",
        "An excerpt from the paper:\n",
        "\n",
        "*\"We encourage the representation to be multiview-consistent by\n",
        "restricting the network to predict the volume density sigma as a\n",
        "function of only the location `x`, while allowing the RGB color `c` to be\n",
        "predicted as a function of both location and viewing direction. To\n",
        "accomplish this, the MLP first processes the input 3D coordinate `x`\n",
        "with 8 fully-connected layers (using ReLU activations and 256 channels\n",
        "per layer), and outputs sigma and a 256-dimensional feature vector.\n",
        "This feature vector is then concatenated with the camera ray's viewing\n",
        "direction and passed to one additional fully-connected layer (using a\n",
        "ReLU activation and 128 channels) that output the view-dependent RGB\n",
        "color.\"*\n",
        "\n",
        "Here we have gone for a minimal implementation and have used 64\n",
        "Dense units instead of 256 as mentioned in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A4EzZHbdgINs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import keras # Added for keras.Input, keras.Model\n",
        "\n",
        "# Assume these are defined globally or passed appropriately\n",
        "# Example values, these should be defined in your main script\n",
        "POS_ENCODE_DIMS_XYZ = 10\n",
        "POS_ENCODE_DIMS_DIR = 4\n",
        "\n",
        "def get_nerf_model(num_layers=8, dense_units=256, skip_layer=4,\n",
        "                   pos_encode_dims_xyz=POS_ENCODE_DIMS_XYZ, # Uses global as default\n",
        "                   pos_encode_dims_dir=POS_ENCODE_DIMS_DIR): # Uses global as default\n",
        "    \"\"\"Defines the NeRF MLP model.\n",
        "\n",
        "    Args:\n",
        "        num_layers: Number of dense layers for position.\n",
        "        dense_units: Number of units in each dense layer.\n",
        "        skip_layer: Layer index for skip connection.\n",
        "        pos_encode_dims_xyz: L for position encoding.\n",
        "        pos_encode_dims_dir: L for direction encoding.\n",
        "\n",
        "    Returns:\n",
        "        A Keras model for NeRF.\n",
        "    \"\"\"\n",
        "    # Input for encoded position (x,y,z)\n",
        "    # Each of x, y, z is encoded to 1 (raw) + 2*L dimensions.\n",
        "    # For 3 coordinates (x,y,z), it's 3 * (1 + 2*L_xyz)\n",
        "    input_xyz_encoded_shape = (1 + 2 * pos_encode_dims_xyz) * 3\n",
        "    input_dir_encoded_shape = (1 + 2 * pos_encode_dims_dir) * 3\n",
        "\n",
        "    input_xyz = keras.Input(shape=(input_xyz_encoded_shape,), name=\"encoded_xyz\")\n",
        "    input_dir = keras.Input(shape=(input_dir_encoded_shape,), name=\"encoded_dir\")\n",
        "\n",
        "    # Position encoding pathway\n",
        "    x = input_xyz\n",
        "    for i in range(num_layers):\n",
        "        x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
        "        if i == skip_layer:\n",
        "            x = layers.concatenate([x, input_xyz]) # Skip connection\n",
        "\n",
        "    # Sigma (Density) head\n",
        "    # The density sigma is predicted from the features of the position only.\n",
        "    sigma = layers.Dense(1, activation=None, name=\"sigma\")(x) # No activation, will be ReLU'd later in volumetric rendering\n",
        "\n",
        "    # Feature vector for RGB prediction\n",
        "    # This feature vector is then concatenated with the view direction.\n",
        "    feature_vector = layers.Dense(dense_units, activation=None)(x) # As per your diagram/note (often ReLU here)\n",
        "\n",
        "    # Concatenate feature vector with encoded direction\n",
        "    concat_feature_dir = layers.concatenate([feature_vector, input_dir])\n",
        "    rgb_intermediate = layers.Dense(dense_units // 2, activation=\"relu\")(concat_feature_dir)\n",
        "    rgb = layers.Dense(3, activation=\"sigmoid\", name=\"rgb\")(rgb_intermediate) # Sigmoid for [0,1] RGB\n",
        "\n",
        "    # Final output: concatenate RGB and sigma\n",
        "    output = layers.concatenate([rgb, sigma], axis=-1) # Shape: (None, 4)\n",
        "\n",
        "    return keras.Model(inputs=[input_xyz, input_dir], outputs=output, name=\"NeRF_MLP\")\n",
        "\n",
        "\n",
        "def volumetric_rendering(raw_output, t_vals, batch_size, H_dim, W_dim, num_samples_dim):\n",
        "    \"\"\"Performs volumetric rendering on the raw MLP outputs.\n",
        "\n",
        "    Args:\n",
        "        raw_output: Raw output from the MLP (batch_size * H * W * num_samples, 4).\n",
        "                    Contains [rgb_from_model, sigma_from_model].\n",
        "        t_vals: The t values for sampling along each ray (batch_size, H, W, num_samples).\n",
        "        batch_size: The batch size. (Can also be inferred from t_vals or raw_output)\n",
        "        H_dim: Height of the image. (Can also be inferred)\n",
        "        W_dim: Width of the image. (Can also be inferred)\n",
        "        num_samples_dim: Number of samples along each ray. (Can also be inferred)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of:\n",
        "            rgb_map: Rendered RGB image (batch_size, H, W, 3).\n",
        "            depth_map: Rendered depth map (batch_size, H, W).\n",
        "            weights: Volumetric rendering weights (batch_size, H, W, num_samples).\n",
        "    \"\"\"\n",
        "    # Infer dynamic shapes if possible, or ensure consistent inputs\n",
        "    actual_batch_size = tf.shape(t_vals)[0]\n",
        "    actual_h_dim = tf.shape(t_vals)[1]\n",
        "    actual_w_dim = tf.shape(t_vals)[2]\n",
        "    actual_num_samples_dim = tf.shape(t_vals)[3]\n",
        "\n",
        "    # Reshape raw output to (batch_size, H, W, num_samples, 4)\n",
        "    raw_output_reshaped = tf.reshape(\n",
        "        raw_output,\n",
        "        (actual_batch_size, actual_h_dim, actual_w_dim, actual_num_samples_dim, 4)\n",
        "    )\n",
        "\n",
        "    # Extract RGB and sigma\n",
        "    # RGB is already sigmoided by the model in get_nerf_model\n",
        "    rgb = raw_output_reshaped[..., :3]\n",
        "    sigma_a = tf.nn.relu(raw_output_reshaped[..., 3]) # Apply ReLU to raw sigma\n",
        "\n",
        "    # Calculate delta: distance between adjacent samples\n",
        "    # t_vals shape: (batch_size, H, W, num_samples)\n",
        "    delta = t_vals[..., 1:] - t_vals[..., :-1] # (batch_size, H, W, num_samples-1)\n",
        "\n",
        "    # Add a large value for the last interval (delta for the last sample)\n",
        "    delta_last = tf.broadcast_to([1e10], shape=tf.concat([tf.shape(delta)[:-1], [1]], axis=0) )\n",
        "    delta = tf.concat([delta, delta_last], axis=-1) # (batch_size, H, W, num_samples)\n",
        "\n",
        "\n",
        "    # Calculate alpha: opacity for each sample\n",
        "    alpha = 1.0 - tf.exp(-sigma_a * delta) # (batch_size, H, W, num_samples)\n",
        "\n",
        "    # Calculate transmittance T_i\n",
        "    transmittance = tf.math.cumprod(1.0 - alpha + 1e-10, axis=-1, exclusive=True)\n",
        "\n",
        "    # Calculate weights for each sample: w_i = T_i * alpha_i\n",
        "    weights = alpha * transmittance # (batch_size, H, W, num_samples)\n",
        "\n",
        "    # Calculate final RGB color for each ray\n",
        "    rgb_map = tf.reduce_sum(weights[..., None] * rgb, axis=-2) # (batch_size, H, W, 3)\n",
        "\n",
        "    # Calculate depth map\n",
        "    depth_map = tf.reduce_sum(weights * t_vals, axis=-1) # (batch_size, H, W)\n",
        "\n",
        "    return rgb_map, depth_map, weights\n",
        "\n",
        "\n",
        "def sample_pdf(bins, weights, num_samples_fine, det=False):\n",
        "    \"\"\"Hierarchical sampling: sample points from a PDF defined by coarse weights.\n",
        "\n",
        "    Args:\n",
        "        bins: Midpoints of the coarse sampling intervals (batch_size, H, W, num_coarse_samples-1).\n",
        "        weights: Weights from the coarse rendering (batch_size, H, W, num_coarse_samples_for_pdf).\n",
        "                 Should correspond to the intervals in 'bins', so typically weights[..., 1:-1].\n",
        "        num_samples_fine: Number of fine samples per ray.\n",
        "        det: If True, use deterministic sampling (linspace). Otherwise, random.\n",
        "\n",
        "    Returns:\n",
        "        Fine samples t_vals (batch_size, H, W, num_samples_fine).\n",
        "    \"\"\"\n",
        "    weights = weights + 1e-5\n",
        "    pdf = weights / tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
        "    cdf = tf.cumsum(pdf, axis=-1)\n",
        "    cdf = tf.concat([tf.zeros_like(cdf[..., :1]), cdf], axis=-1)\n",
        "\n",
        "    batch_dims_shape = tf.shape(cdf)[:-1]\n",
        "    if det:\n",
        "        u_shape = tf.concat([batch_dims_shape, [num_samples_fine]], axis=0)\n",
        "        u = tf.linspace(0.0, 1.0, num_samples_fine)\n",
        "        u = tf.broadcast_to(u, u_shape)\n",
        "    else:\n",
        "        u_shape = tf.concat([batch_dims_shape, [num_samples_fine]], axis=0)\n",
        "        u = tf.random.uniform(shape=u_shape, dtype=pdf.dtype) # Ensure dtype matches\n",
        "\n",
        "    # tf.searchsorted expects cdf and u to be sorted. u is sorted if det=True.\n",
        "    # If u is random, it's not necessarily sorted but searchsorted still gives meaningful indices for inv transform.\n",
        "    inds = tf.searchsorted(cdf, u, side='right')\n",
        "\n",
        "    below = tf.maximum(0, inds - 1)\n",
        "    above = tf.minimum(tf.shape(cdf)[-1] - 1, inds) # cdf has an extra 0 at the beginning\n",
        "    inds_g = tf.stack([below, above], axis=-1)\n",
        "\n",
        "    # batch_dims for gather should be rank(input_tensor) - 1\n",
        "    # For cdf (B,H,W,N_cdf), batch_dims_rank_cdf = 3\n",
        "    # For bins (B,H,W,N_bins), batch_dims_rank_bins = 3\n",
        "    rank_cdf = tf.rank(cdf)\n",
        "    cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=rank_cdf -1)\n",
        "\n",
        "    rank_bins = tf.rank(bins)\n",
        "    bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=rank_bins -1)\n",
        "\n",
        "\n",
        "    denom = cdf_g[..., 1] - cdf_g[..., 0]\n",
        "    denom = tf.where(denom < 1e-5, tf.ones_like(denom), denom)\n",
        "    t = (u - cdf_g[..., 0]) / denom\n",
        "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "def render_nerf_hierarchical(\n",
        "    coarse_model, fine_model, ray_origins, ray_directions, t_vals_coarse,\n",
        "    num_samples_fine, rand_fine_sampling=True):\n",
        "    \"\"\"\n",
        "    Renders rays using a hierarchical sampling approach (coarse and fine models).\n",
        "    Args:\n",
        "        coarse_model: The coarse NeRF Keras model.\n",
        "        fine_model: The fine NeRF Keras model.\n",
        "        ray_origins: Ray origins (batch_size, H, W, 3).\n",
        "        ray_directions: Ray directions (batch_size, H, W, 3).\n",
        "        t_vals_coarse: Coarse sampling t_vals (batch_size, H, W, num_samples_coarse).\n",
        "        num_samples_fine: Number of samples for the fine model.\n",
        "        rand_fine_sampling: Whether to use random sampling for fine stage during training.\n",
        "    Returns:\n",
        "        A dictionary containing renderings.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(ray_origins)[0]\n",
        "    h_dim = tf.shape(ray_origins)[1]\n",
        "    w_dim = tf.shape(ray_origins)[2]\n",
        "    num_s_coarse = tf.shape(t_vals_coarse)[-1]\n",
        "\n",
        "    # 1. COARSE MODEL RENDERING\n",
        "    pts_coarse = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals_coarse[..., None]\n",
        "    dirs_coarse = tf.broadcast_to(ray_directions[..., None, :], tf.shape(pts_coarse))\n",
        "\n",
        "    pts_coarse_flat = tf.reshape(pts_coarse, [-1, 3])\n",
        "    dirs_coarse_flat = tf.reshape(dirs_coarse, [-1, 3])\n",
        "\n",
        "    encoded_pts_coarse = encode_position(pts_coarse_flat, POS_ENCODE_DIMS_XYZ)\n",
        "    encoded_dirs_coarse = encode_position(dirs_coarse_flat, POS_ENCODE_DIMS_DIR)\n",
        "\n",
        "    raw_output_coarse = coarse_model([encoded_pts_coarse, encoded_dirs_coarse], training=True) # Pass training flag\n",
        "\n",
        "    rgb_map_coarse, depth_map_coarse, weights_coarse = volumetric_rendering(\n",
        "        raw_output_coarse, t_vals_coarse, batch_size, h_dim, w_dim, num_s_coarse\n",
        "    )\n",
        "\n",
        "    # 2. HIERARCHICAL SAMPLING FOR FINE MODEL\n",
        "    mid_t_vals_coarse = 0.5 * (t_vals_coarse[..., :-1] + t_vals_coarse[..., 1:])\n",
        "\n",
        "    # Pass weights corresponding to the midpoints/intervals.\n",
        "    # weights_coarse has shape (B, H, W, num_s_coarse).\n",
        "    # mid_t_vals_coarse has shape (B, H, W, num_s_coarse - 1).\n",
        "    # The weights for PDF should be for these intervals.\n",
        "    # Original NeRF uses weights_coarse[..., 1:-1] if bins are z_vals[...,1:-1].\n",
        "    # Here, bins are mid_t_vals_coarse (Nc-1 intervals).\n",
        "    # weights_coarse for these Nc-1 intervals are often taken as weights_coarse[..., :-1] or a similar slice.\n",
        "    # The sample_pdf function expects weights to match the number of bins.\n",
        "    # If bins = mid_t_vals_coarse (Nc-1), then weights for pdf should be (Nc-1).\n",
        "    # A common choice is weights_coarse[..., 1:-1] (Nc-2 items) if bins are also adjusted,\n",
        "    # or use weights_coarse[..., :-1] if it makes sense for (Nc-1) bins.\n",
        "    # Given the original implementation of sample_pdf, it often uses weights[..., 1:-1] for Nc samples,\n",
        "    # implying Nc-2 bins/PDF entries. Let's align with the provided sample_pdf which uses weights as is,\n",
        "    # but it's typically a slice. If sample_pdf internally handles it, or if weights_coarse[..., 1:-1] is intended\n",
        "    # for the input `weights` argument to `sample_pdf`.\n",
        "    # The `sample_pdf` you provided previously uses `weights` directly (e.g. `weights_coarse[..., 1:-1]`)\n",
        "    # which would be (B, H, W, N_coarse - 2) for the PDF.\n",
        "    # And `bins` would be `mid_t_vals_coarse` (B, H, W, N_coarse - 1). There is a mismatch.\n",
        "    # Let's assume the `weights` passed to `sample_pdf` should match the number of `bins`.\n",
        "    # The most robust way from original NeRF is to use z_vals (t_vals) for bins in sample_pdf and the weights for those.\n",
        "    # For simplicity with your `sample_pdf` structure which expects `bins` as midpoints:\n",
        "    # We need weights for these midpoints. The `weights_coarse` are for `t_vals_coarse`.\n",
        "    # Let's use `weights_coarse[..., :-1]` as an approximation for the weights of the intervals defined by `mid_t_vals_coarse`.\n",
        "    # Or, more commonly, the `weights_coarse` excluding the very first and last are used for the PDF over the *intervals*.\n",
        "    # So, `weights_coarse[..., 1:-1]` is standard for PDF sampling. `bins` should then align.\n",
        "    # The provided `sample_pdf` uses `weights` as input, then computes PDF.\n",
        "    # If `bins` is `mid_t_vals_coarse` (Nc-1 elements), and we want PDF over these intervals.\n",
        "    # `weights_coarse` has Nc elements.\n",
        "    # A common approach: `pdf_weights = weights_coarse[..., 1:-1]`.\n",
        "    # `pdf_bins = 0.5 * (t_vals_coarse[..., :-2] + t_vals_coarse[..., 1:-1])` - this would also be Nc-2.\n",
        "    # Let's stick to the provided sample_pdf and assume `weights_coarse[..., 1:-1]` is the intended input for `weights` argument in `sample_pdf`.\n",
        "\n",
        "    t_vals_fine_sampled = sample_pdf(\n",
        "        bins=mid_t_vals_coarse, # (B, H, W, Nc-1)\n",
        "        weights=weights_coarse[..., 1:-1], # (B, H, W, Nc-2), ensure sample_pdf handles this or adjust bins\n",
        "        num_samples_fine=num_samples_fine,\n",
        "        det=not rand_fine_sampling\n",
        "    )\n",
        "\n",
        "    t_vals_all = tf.sort(tf.concat([t_vals_coarse, t_vals_fine_sampled], axis=-1), axis=-1)\n",
        "    num_s_all = tf.shape(t_vals_all)[-1] # num_s_coarse + num_samples_fine (potentially, if sample_pdf handles shapes)\n",
        "\n",
        "\n",
        "    # 3. FINE MODEL RENDERING\n",
        "    pts_fine = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals_all[..., None]\n",
        "    dirs_fine = tf.broadcast_to(ray_directions[..., None, :], tf.shape(pts_fine))\n",
        "\n",
        "    pts_fine_flat = tf.reshape(pts_fine, [-1, 3])\n",
        "    dirs_fine_flat = tf.reshape(dirs_fine, [-1, 3])\n",
        "\n",
        "    encoded_pts_fine = encode_position(pts_fine_flat, POS_ENCODE_DIMS_XYZ)\n",
        "    encoded_dirs_fine = encode_position(dirs_fine_flat, POS_ENCODE_DIMS_DIR)\n",
        "\n",
        "    raw_output_fine = fine_model([encoded_pts_fine, encoded_dirs_fine], training=True) # Pass training flag\n",
        "\n",
        "    rgb_map_fine, depth_map_fine, _ = volumetric_rendering(\n",
        "        raw_output_fine, t_vals_all, batch_size, h_dim, w_dim, num_s_all\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"rgb_coarse\": rgb_map_coarse,\n",
        "        \"depth_coarse\": depth_map_coarse,\n",
        "        \"rgb_fine\": rgb_map_fine,\n",
        "        \"depth_fine\": depth_map_fine,\n",
        "        \"weights_coarse\": weights_coarse, # Return for debugging or other uses\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giPXy26MgINs"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training step is implemented as part of a custom `keras.Model` subclass\n",
        "so that we can make use of the `model.fit` functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ4FZ9NPgINs",
        "outputId": "a81c9bf3-5c34-410e-ca2d-4fd8ea0bf895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '99' to a shape.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d37cddb72a42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mcoarse_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mfine_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-18c4533c1101>\u001b[0m in \u001b[0;36mget_nerf_model\u001b[0;34m(num_layers, num_pos)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Accurate NeRF MLP with residual connection and full separation between σ and RGB heads.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPOS_ENCODE_DIMS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoded (x, y, z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdir_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPOS_ENCODE_DIMS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoded viewing dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     layer = InputLayer(\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Undefined shapes are not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot convert '{shape}' to a shape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tensorflow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '99' to a shape."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import os # For path creation\n",
        "import glob # For create_gif\n",
        "import imageio.v2 as imageio # For create_gif\n",
        "from tqdm import tqdm # For create_gif\n",
        "import matplotlib.pyplot as plt # For TrainMonitor\n",
        "\n",
        "# Assume get_nerf_model, render_nerf_hierarchical, POS_ENCODE_DIMS_XYZ, etc.\n",
        "# are defined above this block as discussed.\n",
        "# For example:\n",
        "POS_ENCODE_DIMS_XYZ = 10\n",
        "POS_ENCODE_DIMS_DIR = 4\n",
        "NUM_SAMPLES_FINE = 128\n",
        "EPOCHS = 20 # Or your desired value\n",
        "BATCH_SIZE = 5 # Or your desired value\n",
        "\n",
        "\n",
        "class NeRFModel(keras.Model):\n",
        "    def __init__(self, coarse_mlp, fine_mlp, num_samples_fine):\n",
        "        super().__init__()\n",
        "        self.coarse_mlp = coarse_mlp\n",
        "        self.fine_mlp = fine_mlp\n",
        "        self.num_samples_fine = num_samples_fine\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.psnr_tracker = keras.metrics.Mean(name=\"psnr\") # PSNR on fine model output\n",
        "\n",
        "    # Modified compile to accept a single optimizer\n",
        "    def compile(self, optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.optimizer = optimizer # Assign the single optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.psnr_tracker]\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        # `inputs` from tf.data pipeline: (images_batch, (rays_o_batch, rays_d_batch, t_vals_coarse_batch))\n",
        "        target_images, (rays_o, rays_d, t_vals_coarse) = inputs\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Render with hierarchical sampling\n",
        "            # Pass training=True to render_nerf_hierarchical\n",
        "            renderings = render_nerf_hierarchical(\n",
        "                self.coarse_mlp, self.fine_mlp, rays_o, rays_d, t_vals_coarse,\n",
        "                self.num_samples_fine,\n",
        "                rand_fine_sampling=True, # Random fine sampling for training\n",
        "                training=True # Explicitly pass training status\n",
        "            )\n",
        "            rgb_coarse = renderings[\"rgb_coarse\"]\n",
        "            rgb_fine = renderings[\"rgb_fine\"]\n",
        "\n",
        "            # Compute losses\n",
        "            loss_coarse = self.loss_fn(target_images, rgb_coarse)\n",
        "            loss_fine = self.loss_fn(target_images, rgb_fine)\n",
        "            total_loss = loss_coarse + loss_fine # Sum of coarse and fine losses\n",
        "\n",
        "        # Compute gradients and apply to both models' variables\n",
        "        trainable_vars = self.coarse_mlp.trainable_variables + self.fine_mlp.trainable_variables\n",
        "        grads = tape.gradient(total_loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(grads, trainable_vars)) # Use the single optimizer\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(total_loss)\n",
        "        psnr = tf.image.psnr(target_images, rgb_fine, max_val=1.0) # PSNR based on fine model\n",
        "        self.psnr_tracker.update_state(psnr)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_tracker.result()}\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        target_images, (rays_o, rays_d, t_vals_coarse) = inputs\n",
        "\n",
        "        # Pass training=False to render_nerf_hierarchical\n",
        "        renderings = render_nerf_hierarchical(\n",
        "            self.coarse_mlp, self.fine_mlp, rays_o, rays_d, t_vals_coarse,\n",
        "            self.num_samples_fine,\n",
        "            rand_fine_sampling=False, # Deterministic fine sampling for testing\n",
        "            training=False # Explicitly pass training status\n",
        "        )\n",
        "        rgb_coarse = renderings[\"rgb_coarse\"]\n",
        "        rgb_fine = renderings[\"rgb_fine\"]\n",
        "\n",
        "        loss_coarse = self.loss_fn(target_images, rgb_coarse)\n",
        "        loss_fine = self.loss_fn(target_images, rgb_fine)\n",
        "        total_loss = loss_coarse + loss_fine\n",
        "\n",
        "        self.loss_tracker.update_state(total_loss)\n",
        "        psnr = tf.image.psnr(target_images, rgb_fine, max_val=1.0)\n",
        "        self.psnr_tracker.update_state(psnr)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_tracker.result()}\n",
        "\n",
        "    def call(self, inputs, training=False): # training default is False for inference\n",
        "        # `inputs`: (rays_o, rays_d, t_vals_coarse)\n",
        "        rays_o, rays_d, t_vals_coarse = inputs\n",
        "        # Pass the `training` flag to render_nerf_hierarchical\n",
        "        renderings = render_nerf_hierarchical(\n",
        "            self.coarse_mlp, self.fine_mlp, rays_o, rays_d, t_vals_coarse,\n",
        "            self.num_samples_fine,\n",
        "            rand_fine_sampling=training, # rand_fine_sampling depends on training status\n",
        "            training=training # Explicitly pass training status to sub-models\n",
        "        )\n",
        "        # Return fine model output for inference by default\n",
        "        return renderings[\"rgb_fine\"], renderings[\"depth_fine\"]\n",
        "\n",
        "\n",
        "# Instantiate models\n",
        "# Ensure POS_ENCODE_DIMS_XYZ and POS_ENCODE_DIMS_DIR are defined\n",
        "coarse_nerf_mlp = get_nerf_model(\n",
        "    num_layers=8, dense_units=256, skip_layer=4,\n",
        "    pos_encode_dims_xyz=POS_ENCODE_DIMS_XYZ,\n",
        "    pos_encode_dims_dir=POS_ENCODE_DIMS_DIR\n",
        ")\n",
        "fine_nerf_mlp = get_nerf_model(\n",
        "    num_layers=8, dense_units=256, skip_layer=4,\n",
        "    pos_encode_dims_xyz=POS_ENCODE_DIMS_XYZ,\n",
        "    pos_encode_dims_dir=POS_ENCODE_DIMS_DIR\n",
        ")\n",
        "\n",
        "# Combined NeRF model\n",
        "# Ensure NUM_SAMPLES_FINE is defined\n",
        "nerf_system = NeRFModel(coarse_nerf_mlp, fine_nerf_mlp, NUM_SAMPLES_FINE)\n",
        "\n",
        "# Compile the model\n",
        "learning_rate = 5e-4\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_function = keras.losses.MeanSquaredError()\n",
        "\n",
        "# Compile with the single optimizer\n",
        "nerf_system.compile(optimizer=optimizer, loss_fn=loss_function)\n",
        "\n",
        "\n",
        "loss_list_train = [] # For plotting training loss\n",
        "\n",
        "# Get a single batch from validation set for visualization during training\n",
        "# Ensure val_ds is defined and is a tf.data.Dataset\n",
        "val_iter = iter(val_ds)\n",
        "val_batch_for_viz = next(val_iter)\n",
        "\n",
        "class TrainMonitor(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None: # logs might be None if training is interrupted early\n",
        "             loss_list_train.append(logs.get(\"loss\")) # Use .get for safety\n",
        "\n",
        "        target_images_viz, (rays_o_viz, rays_d_viz, t_vals_coarse_viz) = val_batch_for_viz\n",
        "\n",
        "        # Call the model directly (invokes `call` method)\n",
        "        # Pass training=False for inference behavior in the callback\n",
        "        predicted_rgb_fine, predicted_depth_fine = self.model(\n",
        "            (rays_o_viz, rays_d_viz, t_vals_coarse_viz), training=False\n",
        "        )\n",
        "\n",
        "        # Ensure BATCH_SIZE and EPOCHS are defined for plotting\n",
        "        num_images_to_show = min(BATCH_SIZE, 3)\n",
        "        fig, axes = plt.subplots(nrows=num_images_to_show, ncols=4, figsize=(20, 5 * num_images_to_show))\n",
        "        if num_images_to_show == 1:\n",
        "            axes = np.array([axes]) # Ensure axes is always 2D for consistent indexing\n",
        "\n",
        "        for i in range(num_images_to_show):\n",
        "            ax_row = axes[i] if num_images_to_show > 1 else axes # Handle single row case\n",
        "\n",
        "            # Ensure target_images_viz[i] is valid for array_to_img\n",
        "            if hasattr(target_images_viz[i], 'numpy'):\n",
        "                target_img_display = target_images_viz[i].numpy()\n",
        "            else:\n",
        "                target_img_display = target_images_viz[i]\n",
        "            ax_row[0].imshow(keras.utils.array_to_img(target_img_display))\n",
        "            ax_row[0].set_title(f\"Target Image {i}\")\n",
        "\n",
        "            if hasattr(predicted_rgb_fine[i], 'numpy'):\n",
        "                rgb_fine_display = predicted_rgb_fine[i].numpy()\n",
        "            else:\n",
        "                rgb_fine_display = predicted_rgb_fine[i]\n",
        "            ax_row[1].imshow(keras.utils.array_to_img(rgb_fine_display))\n",
        "            ax_row[1].set_title(f\"Predicted Fine RGB {i} (Epoch: {epoch:03d})\")\n",
        "\n",
        "            if hasattr(predicted_depth_fine[i], 'numpy'):\n",
        "                depth_fine_display = predicted_depth_fine[i].numpy()\n",
        "            else:\n",
        "                depth_fine_display = predicted_depth_fine[i]\n",
        "            ax_row[2].imshow(keras.utils.array_to_img(depth_fine_display[..., None]), cmap='inferno')\n",
        "            ax_row[2].set_title(f\"Predicted Fine Depth {i} (Epoch: {epoch:03d})\")\n",
        "\n",
        "        # Plot loss\n",
        "        loss_ax = axes[0,3] if num_images_to_show > 1 else axes[3]\n",
        "        loss_ax.plot(loss_list_train)\n",
        "        loss_ax.set_xticks(np.arange(0, EPOCHS + 1, 5.0 if EPOCHS >=5 else 1.0))\n",
        "        loss_ax.set_title(f\"Training Loss (Epoch: {epoch:03d})\")\n",
        "\n",
        "        if num_images_to_show > 1:\n",
        "            for j in range(1, num_images_to_show):\n",
        "                axes[j,3].axis('off') # Hide extra loss plot axes\n",
        "\n",
        "        save_dir = \"images_original_nerf\"\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        fig.savefig(os.path.join(save_dir, f\"{epoch:03d}.png\"))\n",
        "        # plt.show() # Uncomment for interactive display if not in a headless environment\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "# Create a directory to save the images during training.\n",
        "if not os.path.exists(\"images_original_nerf\"):\n",
        "    os.makedirs(\"images_original_nerf\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "# Ensure train_ds, val_ds, EPOCHS are defined.\n",
        "nerf_system.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[TrainMonitor()]\n",
        ")\n",
        "print(\"Training finished.\")\n",
        "\n",
        "\n",
        "def create_gif(path_to_images_pattern, name_gif):\n",
        "    filenames = glob.glob(path_to_images_pattern)\n",
        "    filenames = sorted(filenames)\n",
        "    generated_images = []\n",
        "    if not filenames:\n",
        "        print(f\"No images found for pattern {path_to_images_pattern}\")\n",
        "        return\n",
        "\n",
        "    for filename in tqdm(filenames):\n",
        "        try:\n",
        "            generated_images.append(imageio.imread(filename))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not generated_images:\n",
        "        print(f\"No valid images were read to create GIF.\")\n",
        "        return\n",
        "\n",
        "    kargs = {\"duration\": 0.25, \"loop\": 0}\n",
        "    try:\n",
        "        imageio.mimsave(name_gif, generated_images, \"GIF\", **kargs)\n",
        "        print(f\"GIF saved as {name_gif}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating GIF: {e}\")\n",
        "\n",
        "\n",
        "create_gif(\"images_original_nerf/*.png\", \"training_original_nerf.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Inference and Video Generation (Similar to provided code, adapted for the new model) ---\n",
        "\n",
        "# Get a batch from the test set for inference visualization\n",
        "test_imgs_viz, (test_rays_o_viz, test_rays_d_viz, test_t_vals_coarse_viz) = next(iter(val_ds)) # Using val_ds as test here\n",
        "\n",
        "# Infer with the trained model\n",
        "recons_images_fine, depth_maps_fine = nerf_system(\n",
        "    (test_rays_o_viz, test_rays_d_viz, test_t_vals_coarse_viz), training=False\n",
        ")\n",
        "\n",
        "# Create subplots for visualization\n",
        "num_viz_samples = min(BATCH_SIZE, 5)\n",
        "fig, axes = plt.subplots(nrows=num_viz_samples, ncols=3, figsize=(10, 4 * num_viz_samples))\n",
        "if num_viz_samples == 1:\n",
        "    axes = np.array([axes])\n",
        "\n",
        "\n",
        "for i in range(num_viz_samples):\n",
        "    ax_row = axes[i]\n",
        "    ax_row[0].imshow(keras.utils.array_to_img(test_imgs_viz[i]))\n",
        "    ax_row[0].set_title(\"Original\")\n",
        "\n",
        "    ax_row[1].imshow(keras.utils.array_to_img(recons_images_fine[i]))\n",
        "    ax_row[1].set_title(\"Reconstructed (Fine)\")\n",
        "\n",
        "    ax_row[2].imshow(keras.utils.array_to_img(depth_maps_fine[i, ..., None]), cmap=\"inferno\")\n",
        "    ax_row[2].set_title(\"Depth Map (Fine)\")\n",
        "    ax_row[2].axes.get_xaxis().set_visible(False)\n",
        "    ax_row[2].axes.get_yaxis().set_visible(False)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.show() # 로컬에서 실행 시 주석 해제\n",
        "plt.savefig(\"inference_visualization_original_nerf.png\")\n",
        "print(\"Inference visualization saved as inference_visualization_original_nerf.png\")\n",
        "plt.close(fig)\n",
        "\n",
        "# Functions for novel view synthesis video (can be adapted from your original code)\n",
        "def get_translation_t(t):\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "def get_rotation_phi(phi): # around X-axis\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "def get_rotation_theta(theta): # around Y-axis\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = get_translation_t(radius)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    # This is a common transformation to align camera axes with standard conventions\n",
        "    c2w = tf.convert_to_tensor(np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]), dtype=tf.float32) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "print(\"Generating novel view synthesis video...\")\n",
        "rgb_frames = []\n",
        "num_video_frames = 60 # Fewer frames for quicker generation for testing\n",
        "video_batch_size = BATCH_SIZE # Process in batches if memory is an issue\n",
        "\n",
        "for i in tqdm(range(0, num_video_frames, video_batch_size)):\n",
        "    current_batch_size = min(video_batch_size, num_video_frames - i)\n",
        "    batch_rays_o = []\n",
        "    batch_rays_d = []\n",
        "    batch_t_vals_coarse = []\n",
        "\n",
        "    for j in range(current_batch_size):\n",
        "        theta = (i + j) * (360.0 / num_video_frames)\n",
        "        c2w = pose_spherical(theta, -30.0, 4.0) # Example view parameters\n",
        "\n",
        "        ray_origins_single, ray_directions_single = get_rays(H, W, focal, c2w)\n",
        "        # Expand dims to simulate batch for render_flat_rays if it expects it,\n",
        "        # or adjust render_flat_rays. Here, get_rays already returns unbatched HxWx3.\n",
        "        # We will batch them *after* this loop.\n",
        "\n",
        "        # Coarse t_vals for this single pose (needs to be shaped for the model later)\n",
        "        # Here, we ensure t_vals are generated for a single \"image\" view.\n",
        "        # The render_flat_rays will produce t_vals of shape (H, W, N_samples)\n",
        "        # We need to add a batch dim for the model.\n",
        "        _, _, t_vals_coarse_single = render_flat_rays(\n",
        "            ray_origins_single[None,...], ray_directions_single[None,...], # Add batch dim for this call\n",
        "            near=2.0, far=6.0,\n",
        "            num_samples=NUM_SAMPLES_COARSE,\n",
        "            rand=False # No random sampling for video generation\n",
        "        )\n",
        "        # t_vals_coarse_single is (1, H, W, N_samples), remove batch for list append\n",
        "        batch_rays_o.append(ray_origins_single)\n",
        "        batch_rays_d.append(ray_directions_single)\n",
        "        batch_t_vals_coarse.append(tf.squeeze(t_vals_coarse_single, axis=0))\n",
        "\n",
        "\n",
        "    # Stack to create a batch\n",
        "    rays_o_batch_video = tf.stack(batch_rays_o, axis=0) # (current_batch_size, H, W, 3)\n",
        "    rays_d_batch_video = tf.stack(batch_rays_d, axis=0) # (current_batch_size, H, W, 3)\n",
        "    t_vals_coarse_batch_video = tf.stack(batch_t_vals_coarse, axis=0) # (current_batch_size, H, W, N_coarse)\n",
        "\n",
        "    # Infer with the model\n",
        "    rgb_fine_video_batch, _ = nerf_system(\n",
        "        (rays_o_batch_video, rays_d_batch_video, t_vals_coarse_batch_video), training=False\n",
        "    )\n",
        "\n",
        "    # Process and store frames\n",
        "    for k in range(current_batch_size):\n",
        "        img_np = np.clip(255 * rgb_fine_video_batch[k].numpy(), 0.0, 255.0).astype(np.uint8)\n",
        "        rgb_frames.append(img_np)\n",
        "\n",
        "\n",
        "if rgb_frames:\n",
        "    rgb_video_path = \"rgb_video_original_nerf.mp4\"\n",
        "    imageio.mimwrite(rgb_video_path, rgb_frames, fps=30, quality=8, macro_block_size=1) # quality 1-10\n",
        "    print(f\"Novel view video saved as {rgb_video_path}\")\n",
        "else:\n",
        "    print(\"No frames generated for the video.\")"
      ],
      "metadata": {
        "id": "oVv1ovzx586H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nerf",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}