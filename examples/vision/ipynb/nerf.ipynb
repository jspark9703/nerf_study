{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jspark9703/nerf_study/blob/main/examples/vision/ipynb/nerf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3QCYArugINo"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "soYe8_fvgINp"
      },
      "outputs": [],
      "source": [
        "# Setting random seed to obtain reproducible results.\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio.v2 as imageio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize global variables.\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 5\n",
        "NUM_SAMPLES_COARSE = 64 # Original NeRF는 coarse sampling에 64개 사용\n",
        "NUM_SAMPLES_FINE = 128  # Original NeRF는 fine sampling에 128개 사용\n",
        "POS_ENCODE_DIMS_XYZ = 10 # Original NeRF는 xyz에 L=10 사용\n",
        "POS_ENCODE_DIMS_DIR = 4  # Original NeRF는 view direction에 L=4 사용\n",
        "EPOCHS = 20 # 필요에 따라 조정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DHyvuaExgINq",
        "outputId": "6db26486-6738-4f1b-950b-26a279b9aa88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n",
            "\u001b[1m12727482/12727482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download the data if it does not already exist.\n",
        "url = (\n",
        "    \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
        ")\n",
        "data_path = keras.utils.get_file(origin=url)\n",
        "\n",
        "data = np.load(data_path)\n",
        "images = data[\"images\"]\n",
        "im_shape = images.shape\n",
        "(num_images, H, W, _) = images.shape\n",
        "(poses, focal) = (data[\"poses\"], data[\"focal\"])\n",
        "\n",
        "# Plot a random image from the dataset for visualization.\n",
        "# plt.imshow(images[np.random.randint(low=0, high=num_images)])\n",
        "# plt.show() # 로컬 환경에서 실행 시 주석 해제"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in data.files:\n",
        "    print(f\"- {key}: shape={data[key].shape}, dtype={data[key].dtype}\")"
      ],
      "metadata": {
        "id": "D7vTbFP27xfI",
        "outputId": "3150b660-cef0-4761-a557-e3ebbf40c533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- images: shape=(106, 100, 100, 3), dtype=float32\n",
            "- poses: shape=(106, 4, 4), dtype=float32\n",
            "- focal: shape=(), dtype=float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"예시 이미지 픽셀 값:\", images[0, 0, 0])  # 첫 이미지 왼쪽 위 픽셀\n",
        "print(\"예시 카메라 포즈:\\n\", poses[0])          # 첫 이미지의 카메라 포즈\n",
        "print(\"초점 거리 값:\", focal)                   # 초점 거리 값\n"
      ],
      "metadata": {
        "id": "GRk0rx358f3f",
        "outputId": "b815412a-5e07-441c-82b8-5b8a47153085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시 이미지 픽셀 값: [0. 0. 0.]\n",
            "예시 카메라 포즈:\n",
            " [[-9.9990219e-01  4.1922452e-03 -1.3345719e-02 -5.3798322e-02]\n",
            " [-1.3988681e-02 -2.9965907e-01  9.5394367e-01  3.8454704e+00]\n",
            " [-4.6566129e-10  9.5403719e-01  2.9968831e-01  1.2080823e+00]\n",
            " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]\n",
            "초점 거리 값: 138.88887889922103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rN0MJzTXgINr"
      },
      "outputs": [],
      "source": [
        "def encode_position(x, num_encoding_functions):\n",
        "    \"\"\"Encodes the position or direction into its corresponding Fourier feature.\n",
        "\n",
        "    Args:\n",
        "        x: The input coordinate or direction (3D).\n",
        "        num_encoding_functions: The L value for positional encoding.\n",
        "\n",
        "    Returns:\n",
        "        Fourier features tensors of the position or direction.\n",
        "    \"\"\"\n",
        "    positions = [x]\n",
        "    for i in range(num_encoding_functions):\n",
        "        for fn in [tf.sin, tf.cos]:\n",
        "            positions.append(fn((2.0**i) * x))\n",
        "    return tf.concat(positions, axis=-1)\n",
        "\n",
        "\n",
        "def get_rays(height, width, focal, pose):\n",
        "    \"\"\"Computes origin point and direction vector of rays.\n",
        "\n",
        "    Args:\n",
        "        height: Height of the image.\n",
        "        width: Width of the image.\n",
        "        focal: The focal length between the images and the camera.\n",
        "        pose: The pose matrix of the camera (camera-to-world).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of origin point and direction vector for rays.\n",
        "    \"\"\"\n",
        "    # Build a meshgrid for the rays.\n",
        "    i, j = tf.meshgrid(\n",
        "        tf.range(width, dtype=tf.float32),\n",
        "        tf.range(height, dtype=tf.float32),\n",
        "        indexing=\"xy\",\n",
        "    )\n",
        "\n",
        "    # Normalize the x and y coordinates.\n",
        "    # Directions are in camera coordinates.\n",
        "    transformed_i = (i - width * 0.5) / focal\n",
        "    transformed_j = (j - height * 0.5) / focal\n",
        "\n",
        "    # Create the direction unit vectors in camera coordinates.\n",
        "    # (x, y, z) in camera frame: (transformed_i, -transformed_j, -1)\n",
        "    # Negative z because the camera looks along the negative z-axis.\n",
        "    # Negative y because image y-coordinates usually increase downwards.\n",
        "    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)\n",
        "\n",
        "    # Rotate ray directions from camera frame to world frame.\n",
        "    # The rotation part of the pose matrix is pose[:3, :3].\n",
        "    # ray_directions = directions @ pose[:3, :3] # This is incorrect for batch operations\n",
        "    ray_directions = tf.reduce_sum(directions[..., None, :] * pose[:3, :3], axis=-1)\n",
        "\n",
        "\n",
        "    # Origin of all rays is the camera origin in world coordinates.\n",
        "    # This is the translation part of the pose matrix: pose[:3, -1].\n",
        "    ray_origins = tf.broadcast_to(pose[:3, -1], tf.shape(ray_directions))\n",
        "\n",
        "    return (ray_origins, ray_directions)\n",
        "\n",
        "\n",
        "def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):\n",
        "    \"\"\"Generates sample points along rays.\n",
        "\n",
        "    Args:\n",
        "        ray_origins: Origin of the rays (batch_size, H, W, 3).\n",
        "        ray_directions: Direction of the rays (batch_size, H, W, 3).\n",
        "        near: The near bound for sampling.\n",
        "        far: The far bound for sampling.\n",
        "        num_samples: Number of samples per ray.\n",
        "        rand: If True, add random noise to sampling positions.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of:\n",
        "            rays_flat: Sampled points (batch_size * H * W * num_samples, 3).\n",
        "            directions_flat: Ray directions corresponding to each sample point\n",
        "                             (batch_size * H * W * num_samples, 3).\n",
        "            t_vals: The t values for sampling along each ray (batch_size, H, W, num_samples).\n",
        "    \"\"\"\n",
        "    # tf.linspace creates samples from near to far.\n",
        "    t_vals = tf.linspace(near, far, num_samples)\n",
        "    if rand:\n",
        "        # Add some randomness to the sampling intervals.\n",
        "        shape = tf.shape(ray_origins) # (batch_size, H, W, 3)\n",
        "        # noise shape: (batch_size, H, W, num_samples)\n",
        "        noise = tf.random.uniform(shape=tf.concat([shape[:-1],[num_samples]], axis=0)) * (far - near) / num_samples\n",
        "        t_vals = t_vals + noise\n",
        "\n",
        "    # Expand dimensions of t_vals to multiply with ray_directions.\n",
        "    # r(t) = o + t*d\n",
        "    # rays: (batch_size, H, W, num_samples, 3)\n",
        "    rays = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals[..., None, None]\n",
        "    rays_flat = tf.reshape(rays, [-1, 3]) # (batch_size * H * W * num_samples, 3)\n",
        "\n",
        "    # Each sample point on a ray has the same direction as the ray.\n",
        "    # directions: (batch_size, H, W, num_samples, 3)\n",
        "    directions = tf.broadcast_to(ray_directions[..., None, :], tf.shape(rays))\n",
        "    directions_flat = tf.reshape(directions, [-1, 3]) # (batch_size * H * W * num_samples, 3)\n",
        "\n",
        "    # Reshape t_vals to match the batch structure for later use.\n",
        "    t_vals_batched = tf.broadcast_to(t_vals, tf.shape(rays)[ :-1]) # (batch_size, H, W, num_samples)\n",
        "\n",
        "    return rays_flat, directions_flat, t_vals_batched\n",
        "\n",
        "  def map_fn_coarse(pose_and_image):\n",
        "    pose, image = pose_and_image\n",
        "    ray_origins, ray_directions = get_rays(height=H, width=W, focal=focal, pose=pose)\n",
        "    # For training, we sample rays randomly from the image to speed up training.\n",
        "    # Here, for simplicity, we use all rays. Original NeRF selects a subset of rays.\n",
        "    # To implement random ray sampling:\n",
        "    #   1. Flatten ray_origins and ray_directions.\n",
        "    #   2. Randomly select N_rand rays.\n",
        "    #   3. Reshape corresponding image pixels.\n",
        "    # For now, we continue with all rays and will apply batching later.\n",
        "\n",
        "    rays_flat, dirs_flat, t_vals = render_flat_rays(\n",
        "        ray_origins=ray_origins,\n",
        "        ray_directions=ray_directions,\n",
        "        near=2.0,\n",
        "        far=6.0,\n",
        "        num_samples=NUM_SAMPLES_COARSE,\n",
        "        rand=True, # Random sampling during training\n",
        "    )\n",
        "    # Return structure expected by NeRF model's train_step\n",
        "    return (image, (ray_origins, ray_directions, t_vals))\n",
        "\n",
        "# Create the training split.\n",
        "split_index = int(num_images * 0.8)\n",
        "\n",
        "# Split the images into training and validation.\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "# Split the poses into training and validation.\n",
        "train_poses = poses[:split_index]\n",
        "val_poses = poses[split_index:]\n",
        "\n",
        "# Make the training pipeline.\n",
        "train_img_ds = tf.data.Dataset.from_tensor_slices(train_images.astype(np.float32)/255.0) # Normalize images\n",
        "train_pose_ds = tf.data.Dataset.from_tensor_slices(train_poses.astype(np.float32))\n",
        "train_dataset_raw = tf.data.Dataset.zip((train_pose_ds, train_img_ds))\n",
        "\n",
        "# Map poses to rays and t_vals for coarse sampling\n",
        "train_ds = (\n",
        "    train_dataset_raw.map(map_fn_coarse, num_parallel_calls=AUTO)\n",
        "    .shuffle(BATCH_SIZE * 10) # Shuffle before batching\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "\n",
        "# Make the validation pipeline.\n",
        "val_img_ds = tf.data.Dataset.from_tensor_slices(val_images.astype(np.float32)/255.0) # Normalize images\n",
        "val_pose_ds = tf.data.Dataset.from_tensor_slices(val_poses.astype(np.float32))\n",
        "val_dataset_raw = tf.data.Dataset.zip((val_pose_ds, val_img_ds))\n",
        "\n",
        "val_ds = (\n",
        "    val_dataset_raw.map(map_fn_coarse, num_parallel_calls=AUTO) # Use same map_fn, rand=True is ok for val coarse\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images_batch, (ray_origins, ray_directions, t_vals_batch) in train_ds.take(1):\n",
        "    print(\"✅ 이미지 배치\")\n",
        "    print(\"images_batch.shape:\", images_batch.shape)\n",
        "\n",
        "    print(\"\\n✅ 광선 원점 (ray origins)\")\n",
        "    print(\"ray_origins.shape:\", ray_origins.shape)\n",
        "\n",
        "    print(\"\\n✅ 광선 방향 (ray directions)\")\n",
        "    print(\"ray_directions.shape:\", ray_directions.shape)\n",
        "\n",
        "    print(\"\\n✅ 각 샘플 포인트의 깊이 값 (t_vals)\")\n",
        "    print(\"t_vals_batch.shape:\", t_vals_batch.shape)\n"
      ],
      "metadata": {
        "id": "abvJPILW9VFE",
        "outputId": "7f712f9e-b2e0-4ed3-f5a7-3adaeced28b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 이미지 배치\n",
            "images_batch.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 광선 원점 (ray origins)\n",
            "ray_origins.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 광선 방향 (ray directions)\n",
            "ray_directions.shape: (5, 100, 100, 3)\n",
            "\n",
            "✅ 각 샘플 포인트의 깊이 값 (t_vals)\n",
            "t_vals_batch.shape: (5, 100, 100, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxrKm7PegINr"
      },
      "source": [
        "## NeRF model\n",
        "\n",
        "The model is a multi-layer perceptron (MLP), with ReLU as its non-linearity.\n",
        "\n",
        "An excerpt from the paper:\n",
        "\n",
        "*\"We encourage the representation to be multiview-consistent by\n",
        "restricting the network to predict the volume density sigma as a\n",
        "function of only the location `x`, while allowing the RGB color `c` to be\n",
        "predicted as a function of both location and viewing direction. To\n",
        "accomplish this, the MLP first processes the input 3D coordinate `x`\n",
        "with 8 fully-connected layers (using ReLU activations and 256 channels\n",
        "per layer), and outputs sigma and a 256-dimensional feature vector.\n",
        "This feature vector is then concatenated with the camera ray's viewing\n",
        "direction and passed to one additional fully-connected layer (using a\n",
        "ReLU activation and 128 channels) that output the view-dependent RGB\n",
        "color.\"*\n",
        "\n",
        "Here we have gone for a minimal implementation and have used 64\n",
        "Dense units instead of 256 as mentioned in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4EzZHbdgINs"
      },
      "outputs": [],
      "source": [
        "def get_nerf_model(num_layers=8, dense_units=256, skip_layer=4,\n",
        "                   pos_encode_dims_xyz=POS_ENCODE_DIMS_XYZ,\n",
        "                   pos_encode_dims_dir=POS_ENCODE_DIMS_DIR):\n",
        "    \"\"\"Defines the NeRF MLP model.\n",
        "\n",
        "    Args:\n",
        "        num_layers: Number of dense layers for position.\n",
        "        dense_units: Number of units in each dense layer.\n",
        "        skip_layer: Layer index for skip connection.\n",
        "        pos_encode_dims_xyz: L for position encoding.\n",
        "        pos_encode_dims_dir: L for direction encoding.\n",
        "\n",
        "    Returns:\n",
        "        A Keras model for NeRF.\n",
        "    \"\"\"\n",
        "    # Input for encoded position (x,y,z)\n",
        "    # Each of x, y, z is encoded to 2*L + 1 dimensions. So total (2*L_xyz + 1) * 3\n",
        "    # However, the original paper's positional encoding also includes the raw coordinate.\n",
        "    # So, for each coordinate: x, sin(x), cos(x), sin(2x), cos(2x), ..., sin(2^(L-1)x), cos(2^(L-1)x)\n",
        "    # This is 1 (raw) + 2*L dimensions.\n",
        "    # For 3 coordinates (x,y,z), it's 3 * (1 + 2*L_xyz)\n",
        "    input_xyz_encoded_shape = (1 + 2 * pos_encode_dims_xyz) * 3\n",
        "    input_dir_encoded_shape = (1 + 2 * pos_encode_dims_dir) * 3\n",
        "\n",
        "    input_xyz = keras.Input(shape=(input_xyz_encoded_shape), name=\"encoded_xyz\")\n",
        "    input_dir = keras.Input(shape=(input_dir_encoded_shape), name=\"encoded_dir\")\n",
        "\n",
        "    # Position encoding pathway\n",
        "    x = input_xyz\n",
        "    for i in range(num_layers):\n",
        "        x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
        "        if i == skip_layer:\n",
        "            x = layers.concatenate([x, input_xyz]) # Skip connection\n",
        "\n",
        "    # Sigma (Density) head\n",
        "    # The density sigma is predicted from the features of the position only.\n",
        "    sigma = layers.Dense(1, activation=None, name=\"sigma\")(x) # No activation, will be ReLU'd later\n",
        "\n",
        "    # Feature vector for RGB prediction\n",
        "    # This feature vector is then concatenated with the view direction.\n",
        "    feature_vector = layers.Dense(dense_units, activation=None)(x) # No activation here as per diagram\n",
        "\n",
        "    # Concatenate feature vector with encoded direction\n",
        "    concat_feature_dir = layers.concatenate([feature_vector, input_dir])\n",
        "    rgb_intermediate = layers.Dense(dense_units // 2, activation=\"relu\")(concat_feature_dir)\n",
        "    rgb = layers.Dense(3, activation=\"sigmoid\", name=\"rgb\")(rgb_intermediate) # Sigmoid for [0,1] RGB\n",
        "\n",
        "    # Final output: concatenate RGB and sigma\n",
        "    output = layers.concatenate([rgb, sigma], axis=-1) # Shape: (batch, 4)\n",
        "\n",
        "    return keras.Model(inputs=[input_xyz, input_dir], outputs=output, name=\"NeRF_MLP\")\n",
        "\n",
        "\n",
        "def volumetric_rendering(raw_output, t_vals, batch_size, H_dim, W_dim, num_samples_dim):\n",
        "    \"\"\"Performs volumetric rendering on the raw MLP outputs.\n",
        "\n",
        "    Args:\n",
        "        raw_output: Raw output from the MLP (batch_size * H * W * num_samples, 4).\n",
        "                    Contains [rgb, sigma].\n",
        "        t_vals: The t values for sampling along each ray (batch_size, H, W, num_samples).\n",
        "        batch_size: The batch size.\n",
        "        H_dim: Height of the image.\n",
        "        W_dim: Width of the image.\n",
        "        num_samples_dim: Number of samples along each ray.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of:\n",
        "            rgb_map: Rendered RGB image (batch_size, H, W, 3).\n",
        "            depth_map: Rendered depth map (batch_size, H, W).\n",
        "            weights: Volumetric rendering weights (batch_size, H, W, num_samples).\n",
        "    \"\"\"\n",
        "    # Reshape raw output to (batch_size, H, W, num_samples, 4)\n",
        "    raw_output_reshaped = tf.reshape(raw_output, (batch_size, H_dim, W_dim, num_samples_dim, 4))\n",
        "\n",
        "    # Extract RGB and sigma\n",
        "    rgb = tf.sigmoid(raw_output_reshaped[..., :3]) # Apply sigmoid to raw RGB logits if not already applied\n",
        "    sigma_a = tf.nn.relu(raw_output_reshaped[..., 3]) # Apply ReLU to raw sigma\n",
        "\n",
        "    # Calculate delta: distance between adjacent samples\n",
        "    # t_vals shape: (batch_size, H, W, num_samples)\n",
        "    delta = t_vals[..., 1:] - t_vals[..., :-1] # (batch_size, H, W, num_samples-1)\n",
        "\n",
        "    # Add a large value for the last interval (delta for the last sample)\n",
        "    # This approximates infinity, meaning the ray travels indefinitely after the last sample.\n",
        "    delta = tf.concat(\n",
        "        [delta, tf.broadcast_to([1e10], shape=(batch_size, H_dim, W_dim, 1))], axis=-1\n",
        "    ) # (batch_size, H, W, num_samples)\n",
        "\n",
        "    # Calculate alpha: opacity for each sample\n",
        "    # alpha = 1 - exp(-sigma_a * delta)\n",
        "    alpha = 1.0 - tf.exp(-sigma_a * delta) # (batch_size, H, W, num_samples)\n",
        "\n",
        "    # Calculate transmittance T_i: probability that the ray travels from near plane to sample i\n",
        "    # without hitting anything.\n",
        "    # T_i = product_{j=1 to i-1} (1 - alpha_j)\n",
        "    # tf.math.cumprod computes cumulative product. exclusive=True means T_1 = 1.\n",
        "    transmittance = tf.math.cumprod(1.0 - alpha + 1e-10, axis=-1, exclusive=True) # Add epsilon for stability\n",
        "\n",
        "    # Calculate weights for each sample: w_i = T_i * alpha_i\n",
        "    weights = alpha * transmittance # (batch_size, H, W, num_samples)\n",
        "\n",
        "    # Calculate final RGB color for each ray (weighted sum of sample colors)\n",
        "    # rgb_map = sum_{i=1 to N} w_i * c_i\n",
        "    rgb_map = tf.reduce_sum(weights[..., None] * rgb, axis=-2) # (batch_size, H, W, 3)\n",
        "\n",
        "    # Calculate depth map (expected depth along each ray)\n",
        "    # depth_map = sum_{i=1 to N} w_i * t_i\n",
        "    depth_map = tf.reduce_sum(weights * t_vals, axis=-1) # (batch_size, H, W)\n",
        "\n",
        "    return rgb_map, depth_map, weights\n",
        "\n",
        "\n",
        "def sample_pdf(bins, weights, num_samples_fine, det=False):\n",
        "    \"\"\"Hierarchical sampling: sample points from a PDF defined by coarse weights.\n",
        "\n",
        "    Args:\n",
        "        bins: Midpoints of the coarse sampling intervals (batch_size, H, W, num_coarse_samples-1).\n",
        "        weights: Weights from the coarse rendering (batch_size, H, W, num_coarse_samples).\n",
        "                 We use weights[:, :, :, 1:-1] corresponding to the bins.\n",
        "        num_samples_fine: Number of fine samples per ray.\n",
        "        det: If True, use deterministic sampling (linspace). Otherwise, random.\n",
        "\n",
        "    Returns:\n",
        "        Fine samples t_vals (batch_size, H, W, num_samples_fine).\n",
        "    \"\"\"\n",
        "    # Add a small epsilon to weights to prevent NaNs when all weights are zero.\n",
        "    weights = weights + 1e-5\n",
        "    # Normalize weights to get a PDF.\n",
        "    # pdf shape: (batch_size, H, W, num_coarse_samples-2)\n",
        "    pdf = weights / tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # Calculate CDF (Cumulative Distribution Function).\n",
        "    # cdf shape: (batch_size, H, W, num_coarse_samples-2)\n",
        "    cdf = tf.cumsum(pdf, axis=-1)\n",
        "    # Prepend zeros to the CDF to handle edge cases in searchsorted.\n",
        "    # cdf shape: (batch_size, H, W, num_coarse_samples-1)\n",
        "    cdf = tf.concat([tf.zeros_like(cdf[..., :1]), cdf], axis=-1)\n",
        "\n",
        "\n",
        "    # Generate uniform samples for inverse transform sampling.\n",
        "    batch_dims = tf.shape(cdf)[:-1] # (batch_size, H, W)\n",
        "    if det: # Deterministic sampling\n",
        "        u = tf.linspace(0.0, 1.0, num_samples_fine)\n",
        "        u = tf.broadcast_to(u, tf.concat([batch_dims, [num_samples_fine]], axis=0))\n",
        "    else: # Random sampling\n",
        "        u = tf.random.uniform(shape=tf.concat([batch_dims, [num_samples_fine]], axis=0))\n",
        "\n",
        "    # `tf.searchsorted` finds the indices where elements of `u` would be inserted\n",
        "    # into `cdf` to maintain order. This is the core of inverse transform sampling.\n",
        "    # `inds` will have shape (batch_size, H, W, num_samples_fine)\n",
        "    inds = tf.searchsorted(cdf, u, side='right')\n",
        "\n",
        "    # Correct indices to be within bounds of `bins`.\n",
        "    below = tf.maximum(0, inds - 1)\n",
        "    above = tf.minimum(tf.shape(cdf)[-1] - 1, inds)\n",
        "    inds_g = tf.stack([below, above], axis=-1) # (batch_size, H, W, num_samples_fine, 2)\n",
        "\n",
        "    # Gather corresponding CDF values and bin values using the sampled indices.\n",
        "    # cdf_g and bins_g will have shape (batch_size, H, W, num_samples_fine, 2)\n",
        "    cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=tf.rank(cdf)-1) # batch_dims = 3 for (B,H,W,N_cdf)\n",
        "    bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=tf.rank(bins)-1) # batch_dims = 3 for (B,H,W,N_bins)\n",
        "\n",
        "    # Perform linear interpolation to get the fine sample t_vals.\n",
        "    denom = cdf_g[..., 1] - cdf_g[..., 0]\n",
        "    # Avoid division by zero if cdf_g[..., 1] == cdf_g[..., 0]\n",
        "    denom = tf.where(denom < 1e-5, tf.ones_like(denom), denom)\n",
        "    t = (u - cdf_g[..., 0]) / denom\n",
        "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
        "\n",
        "    return samples # (batch_size, H, W, num_samples_fine)\n",
        "\n",
        "\n",
        "def render_nerf_hierarchical(\n",
        "    coarse_model, fine_model, ray_origins, ray_directions, t_vals_coarse,\n",
        "    num_samples_fine, rand_fine_sampling=True):\n",
        "    \"\"\"\n",
        "    Renders rays using a hierarchical sampling approach (coarse and fine models).\n",
        "\n",
        "    Args:\n",
        "        coarse_model: The coarse NeRF Keras model.\n",
        "        fine_model: The fine NeRF Keras model.\n",
        "        ray_origins: Ray origins (batch_size, H, W, 3).\n",
        "        ray_directions: Ray directions (batch_size, H, W, 3).\n",
        "        t_vals_coarse: Coarse sampling t_vals (batch_size, H, W, num_samples_coarse).\n",
        "        num_samples_fine: Number of samples for the fine model.\n",
        "        rand_fine_sampling: Whether to use random sampling for fine stage during training.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing:\n",
        "            'rgb_coarse': Rendered RGB from coarse model.\n",
        "            'depth_coarse': Rendered depth from coarse model.\n",
        "            'rgb_fine': Rendered RGB from fine model.\n",
        "            'depth_fine': Rendered depth from fine model.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(ray_origins)[0]\n",
        "    h_dim = tf.shape(ray_origins)[1]\n",
        "    w_dim = tf.shape(ray_origins)[2]\n",
        "    num_samples_coarse = tf.shape(t_vals_coarse)[-1]\n",
        "\n",
        "    # 1. COARSE MODEL RENDERING\n",
        "    # Generate points and directions for coarse model input\n",
        "    # pts_coarse: (batch_size, H, W, num_samples_coarse, 3)\n",
        "    pts_coarse = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals_coarse[..., None]\n",
        "    dirs_coarse = tf.broadcast_to(ray_directions[..., None, :], tf.shape(pts_coarse))\n",
        "\n",
        "    pts_coarse_flat = tf.reshape(pts_coarse, [-1, 3])\n",
        "    dirs_coarse_flat = tf.reshape(dirs_coarse, [-1, 3])\n",
        "\n",
        "    # Encode inputs\n",
        "    encoded_pts_coarse = encode_position(pts_coarse_flat, POS_ENCODE_DIMS_XYZ)\n",
        "    encoded_dirs_coarse = encode_position(dirs_coarse_flat, POS_ENCODE_DIMS_DIR)\n",
        "\n",
        "    # Predict with coarse model\n",
        "    raw_output_coarse = coarse_model([encoded_pts_coarse, encoded_dirs_coarse])\n",
        "\n",
        "    # Volumetric rendering for coarse model\n",
        "    rgb_map_coarse, depth_map_coarse, weights_coarse = volumetric_rendering(\n",
        "        raw_output_coarse, t_vals_coarse, batch_size, h_dim, w_dim, num_samples_coarse\n",
        "    )\n",
        "\n",
        "    # 2. HIERARCHICAL SAMPLING FOR FINE MODEL\n",
        "    # Get midpoints of coarse sample intervals for PDF sampling\n",
        "    # t_vals_coarse shape: (B, H, W, Nc)\n",
        "    # mid_t_vals shape: (B, H, W, Nc-1)\n",
        "    mid_t_vals_coarse = 0.5 * (t_vals_coarse[..., :-1] + t_vals_coarse[..., 1:])\n",
        "\n",
        "    # Sample fine t_vals based on coarse weights\n",
        "    # weights_coarse shape: (B, H, W, Nc)\n",
        "    # We use weights[:, :, :, 1:-1] for the PDF, corresponding to mid_t_vals_coarse\n",
        "    t_vals_fine_sampled = sample_pdf(\n",
        "        mid_t_vals_coarse,\n",
        "        weights_coarse[..., 1:-1], # Use weights corresponding to intervals\n",
        "        num_samples_fine,\n",
        "        det=not rand_fine_sampling # Use deterministic for testing, random for training\n",
        "    )\n",
        "    # t_vals_fine_sampled shape: (B, H, W, Nf)\n",
        "\n",
        "    # Combine coarse and fine t_vals and sort them\n",
        "    # t_vals_all shape: (B, H, W, Nc + Nf)\n",
        "    t_vals_all = tf.sort(tf.concat([t_vals_coarse, t_vals_fine_sampled], axis=-1), axis=-1)\n",
        "    num_samples_all = num_samples_coarse + num_samples_fine\n",
        "\n",
        "    # 3. FINE MODEL RENDERING\n",
        "    # Generate points and directions for fine model input using all t_vals\n",
        "    # pts_fine: (batch_size, H, W, num_samples_all, 3)\n",
        "    pts_fine = ray_origins[..., None, :] + ray_directions[..., None, :] * t_vals_all[..., None]\n",
        "    dirs_fine = tf.broadcast_to(ray_directions[..., None, :], tf.shape(pts_fine))\n",
        "\n",
        "    pts_fine_flat = tf.reshape(pts_fine, [-1, 3])\n",
        "    dirs_fine_flat = tf.reshape(dirs_fine, [-1, 3])\n",
        "\n",
        "    # Encode inputs\n",
        "    encoded_pts_fine = encode_position(pts_fine_flat, POS_ENCODE_DIMS_XYZ)\n",
        "    encoded_dirs_fine = encode_position(dirs_fine_flat, POS_ENCODE_DIMS_DIR)\n",
        "\n",
        "    # Predict with fine model\n",
        "    raw_output_fine = fine_model([encoded_pts_fine, encoded_dirs_fine])\n",
        "\n",
        "    # Volumetric rendering for fine model\n",
        "    rgb_map_fine, depth_map_fine, _ = volumetric_rendering(\n",
        "        raw_output_fine, t_vals_all, batch_size, h_dim, w_dim, num_samples_all\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"rgb_coarse\": rgb_map_coarse,\n",
        "        \"depth_coarse\": depth_map_coarse,\n",
        "        \"rgb_fine\": rgb_map_fine,\n",
        "        \"depth_fine\": depth_map_fine,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giPXy26MgINs"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training step is implemented as part of a custom `keras.Model` subclass\n",
        "so that we can make use of the `model.fit` functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ4FZ9NPgINs",
        "outputId": "a81c9bf3-5c34-410e-ca2d-4fd8ea0bf895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '99' to a shape.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d37cddb72a42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mcoarse_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mfine_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-18c4533c1101>\u001b[0m in \u001b[0;36mget_nerf_model\u001b[0;34m(num_layers, num_pos)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nerf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Accurate NeRF MLP with residual connection and full separation between σ and RGB heads.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPOS_ENCODE_DIMS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoded (x, y, z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdir_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPOS_ENCODE_DIMS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoded viewing dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     layer = InputLayer(\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Undefined shapes are not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot convert '{shape}' to a shape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tensorflow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '99' to a shape."
          ]
        }
      ],
      "source": [
        "class NeRFModel(keras.Model):\n",
        "    def __init__(self, coarse_mlp, fine_mlp, num_samples_fine):\n",
        "        super().__init__()\n",
        "        self.coarse_mlp = coarse_mlp\n",
        "        self.fine_mlp = fine_mlp\n",
        "        self.num_samples_fine = num_samples_fine\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.psnr_tracker = keras.metrics.Mean(name=\"psnr\") # PSNR on fine model output\n",
        "\n",
        "    def compile(self, optimizer_coarse, optimizer_fine, loss_fn):\n",
        "        super().compile()\n",
        "        # Original NeRF uses the same optimizer for both, but we can have separate if needed\n",
        "        self.optimizer_coarse = optimizer_coarse\n",
        "        self.optimizer_fine = optimizer_fine\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.psnr_tracker]\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        # `inputs` from tf.data pipeline: (images_batch, (rays_o_batch, rays_d_batch, t_vals_coarse_batch))\n",
        "        target_images, (rays_o, rays_d, t_vals_coarse) = inputs\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Render with hierarchical sampling\n",
        "            renderings = render_nerf_hierarchical(\n",
        "                self.coarse_mlp, self.fine_mlp, rays_o, rays_d, t_vals_coarse,\n",
        "                self.num_samples_fine, rand_fine_sampling=True # Random fine sampling for training\n",
        "            )\n",
        "            rgb_coarse = renderings[\"rgb_coarse\"]\n",
        "            rgb_fine = renderings[\"rgb_fine\"]\n",
        "\n",
        "            # Compute losses\n",
        "            loss_coarse = self.loss_fn(target_images, rgb_coarse)\n",
        "            loss_fine = self.loss_fn(target_images, rgb_fine)\n",
        "            total_loss = loss_coarse + loss_fine # Sum of coarse and fine losses\n",
        "\n",
        "        # Compute gradients and apply to both models\n",
        "        trainable_vars = self.coarse_mlp.trainable_variables + self.fine_mlp.trainable_variables\n",
        "        grads = tape.gradient(total_loss, trainable_vars)\n",
        "        # Assuming a single optimizer for both for simplicity, or adapt if using two\n",
        "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
        "\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(total_loss)\n",
        "        psnr = tf.image.psnr(target_images, rgb_fine, max_val=1.0) # PSNR based on fine model\n",
        "        self.psnr_tracker.update_state(psnr)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_tracker.result()}\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        target_images, (rays_o, rays_d, t_vals_coarse) = inputs\n",
        "\n",
        "        renderings = render_nerf_hierarchical(\n",
        "            self.coarse_mlp, self.fine_mlp, rays_o, rays_d, t_vals_coarse,\n",
        "            self.num_samples_fine, rand_fine_sampling=False # Deterministic fine sampling for testing\n",
        "        )\n",
        "        rgb_coarse = renderings[\"rgb_coarse\"]\n",
        "        rgb_fine = renderings[\"rgb_fine\"]\n",
        "\n",
        "        loss_coarse = self.loss_fn(target_images, rgb_coarse)\n",
        "        loss_fine = self.loss_fn(target_images, rgb_fine)\n",
        "        total_loss = loss_coarse + loss_fine\n",
        "\n",
        "        self.loss_tracker.update_state(total_loss)\n",
        "        psnr = tf.image.psnr(target_images, rgb_fine, max_val=1.0)\n",
        "        self.psnr_tracker.update_state(psnr)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_tracker.result()}\n",
        "\n",
        "    # Add a predict_step or call method for inference if needed outside of fit/evaluate\n",
        "    def call(self, inputs, training=False):\n",
        "        # `inputs`: (rays_o, rays_d, t_vals_coarse)\n",
        "        rays_o, rays_d, t_vals_coarse = inputs\n",
        "        renderings = render_nerf_hierarchical(\n",
        "            self.coarse_mlp, self.fine_mlp, rays_o, rays_d, t_vals_coarse,\n",
        "            self.num_samples_fine, rand_fine_sampling=training\n",
        "        )\n",
        "        # Return fine model output for inference by default\n",
        "        return renderings[\"rgb_fine\"], renderings[\"depth_fine\"]\n",
        "\n",
        "\n",
        "# Instantiate models\n",
        "coarse_nerf_mlp = get_nerf_model(\n",
        "    num_layers=8, dense_units=256, skip_layer=4,\n",
        "    pos_encode_dims_xyz=POS_ENCODE_DIMS_XYZ,\n",
        "    pos_encode_dims_dir=POS_ENCODE_DIMS_DIR\n",
        ")\n",
        "fine_nerf_mlp = get_nerf_model(\n",
        "    num_layers=8, dense_units=256, skip_layer=4,\n",
        "    pos_encode_dims_xyz=POS_ENCODE_DIMS_XYZ,\n",
        "    pos_encode_dims_dir=POS_ENCODE_DIMS_DIR\n",
        ")\n",
        "\n",
        "# Combined NeRF model\n",
        "nerf_system = NeRFModel(coarse_nerf_mlp, fine_nerf_mlp, NUM_SAMPLES_FINE)\n",
        "\n",
        "# Compile the model\n",
        "# Original NeRF uses Adam optimizer with lr 5e-4, decaying\n",
        "learning_rate = 5e-4 # Can add learning rate schedule later\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_function = keras.losses.MeanSquaredError()\n",
        "\n",
        "# If using separate optimizers:\n",
        "# optimizer_coarse = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "# optimizer_fine = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "# nerf_system.compile(optimizer_coarse=optimizer_coarse, optimizer_fine=optimizer_fine, loss_fn=loss_function)\n",
        "\n",
        "nerf_system.compile(optimizer=optimizer, loss_fn=loss_function) # Use this if NeRFModel's train_step uses self.optimizer\n",
        "\n",
        "\n",
        "loss_list_train = [] # For plotting training loss\n",
        "\n",
        "# Get a single batch from validation set for visualization during training\n",
        "val_iter = iter(val_ds)\n",
        "val_batch_for_viz = next(val_iter)\n",
        "\n",
        "class TrainMonitor(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        loss_list_train.append(logs[\"loss\"])\n",
        "\n",
        "        # Use the NeRFModel's call method for inference\n",
        "        target_images_viz, (rays_o_viz, rays_d_viz, t_vals_coarse_viz) = val_batch_for_viz\n",
        "\n",
        "        # Reshape t_vals_coarse_viz if it's flattened by map_fn before batching\n",
        "        # Ensure t_vals_coarse_viz has shape (BATCH_SIZE, H, W, NUM_SAMPLES_COARSE)\n",
        "        # The map_fn currently returns (ray_origins, ray_directions, t_vals)\n",
        "        # where t_vals is already (B, H, W, N_samples_coarse) from render_flat_rays\n",
        "        # So, no explicit reshape might be needed here if map_fn and batching are correct.\n",
        "\n",
        "        predicted_rgb_fine, predicted_depth_fine = self.model(\n",
        "            (rays_o_viz, rays_d_viz, t_vals_coarse_viz), training=False\n",
        "        )\n",
        "\n",
        "        # Plot the rgb, depth and the loss plot.\n",
        "        # Display for the first image in the validation batch\n",
        "        num_images_to_show = min(BATCH_SIZE, 3) # Show up to 3 images\n",
        "        fig, axes = plt.subplots(nrows=num_images_to_show, ncols=4, figsize=(20, 5 * num_images_to_show))\n",
        "        if num_images_to_show == 1: # if axes is not an array of arrays\n",
        "            axes = np.array([axes])\n",
        "\n",
        "\n",
        "        for i in range(num_images_to_show):\n",
        "            ax_row = axes[i]\n",
        "            ax_row[0].imshow(keras.utils.array_to_img(target_images_viz[i]))\n",
        "            ax_row[0].set_title(f\"Target Image {i}\")\n",
        "\n",
        "            ax_row[1].imshow(keras.utils.array_to_img(predicted_rgb_fine[i]))\n",
        "            ax_row[1].set_title(f\"Predicted Fine RGB {i} (Epoch: {epoch:03d})\")\n",
        "\n",
        "            ax_row[2].imshow(keras.utils.array_to_img(predicted_depth_fine[i, ..., None]))\n",
        "            ax_row[2].set_title(f\"Predicted Fine Depth {i} (Epoch: {epoch:03d})\")\n",
        "\n",
        "        # Plot loss on the last subplot of the first row (or a dedicated one)\n",
        "        axes[0,3].plot(loss_list_train)\n",
        "        axes[0,3].set_xticks(np.arange(0, EPOCHS + 1, 5.0))\n",
        "        axes[0,3].set_title(f\"Training Loss (Epoch: {epoch:03d})\")\n",
        "        for j in range(1, num_images_to_show): # Hide extra loss plots\n",
        "             axes[j,3].axis('off')\n",
        "\n",
        "\n",
        "        if not os.path.exists(\"images_original_nerf\"):\n",
        "            os.makedirs(\"images_original_nerf\")\n",
        "        fig.savefig(f\"images_original_nerf/{epoch:03d}.png\")\n",
        "        # plt.show() # 로컬에서 실행 시 주석 해제\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "# Create a directory to save the images during training.\n",
        "if not os.path.exists(\"images_original_nerf\"):\n",
        "    os.makedirs(\"images_original_nerf\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "nerf_system.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[TrainMonitor()]\n",
        ")\n",
        "print(\"Training finished.\")\n",
        "\n",
        "\n",
        "def create_gif(path_to_images_pattern, name_gif):\n",
        "    filenames = glob.glob(path_to_images_pattern)\n",
        "    filenames = sorted(filenames)\n",
        "    generated_images = []\n",
        "    for filename in tqdm(filenames):\n",
        "        try:\n",
        "            generated_images.append(imageio.imread(filename))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filename}: {e}\")\n",
        "            continue # Skip problematic files\n",
        "\n",
        "    if not generated_images:\n",
        "        print(f\"No images found for pattern {path_to_images_pattern} or all were problematic.\")\n",
        "        return\n",
        "\n",
        "    kargs = {\"duration\": 0.25, \"loop\": 0} # Loop indefinitely\n",
        "    try:\n",
        "        imageio.mimsave(name_gif, generated_images, \"GIF\", **kargs)\n",
        "        print(f\"GIF saved as {name_gif}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating GIF: {e}\")\n",
        "\n",
        "\n",
        "create_gif(\"images_original_nerf/*.png\", \"training_original_nerf.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Inference and Video Generation (Similar to provided code, adapted for the new model) ---\n",
        "\n",
        "# Get a batch from the test set for inference visualization\n",
        "test_imgs_viz, (test_rays_o_viz, test_rays_d_viz, test_t_vals_coarse_viz) = next(iter(val_ds)) # Using val_ds as test here\n",
        "\n",
        "# Infer with the trained model\n",
        "recons_images_fine, depth_maps_fine = nerf_system(\n",
        "    (test_rays_o_viz, test_rays_d_viz, test_t_vals_coarse_viz), training=False\n",
        ")\n",
        "\n",
        "# Create subplots for visualization\n",
        "num_viz_samples = min(BATCH_SIZE, 5)\n",
        "fig, axes = plt.subplots(nrows=num_viz_samples, ncols=3, figsize=(10, 4 * num_viz_samples))\n",
        "if num_viz_samples == 1:\n",
        "    axes = np.array([axes])\n",
        "\n",
        "\n",
        "for i in range(num_viz_samples):\n",
        "    ax_row = axes[i]\n",
        "    ax_row[0].imshow(keras.utils.array_to_img(test_imgs_viz[i]))\n",
        "    ax_row[0].set_title(\"Original\")\n",
        "\n",
        "    ax_row[1].imshow(keras.utils.array_to_img(recons_images_fine[i]))\n",
        "    ax_row[1].set_title(\"Reconstructed (Fine)\")\n",
        "\n",
        "    ax_row[2].imshow(keras.utils.array_to_img(depth_maps_fine[i, ..., None]), cmap=\"inferno\")\n",
        "    ax_row[2].set_title(\"Depth Map (Fine)\")\n",
        "    ax_row[2].axes.get_xaxis().set_visible(False)\n",
        "    ax_row[2].axes.get_yaxis().set_visible(False)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.show() # 로컬에서 실행 시 주석 해제\n",
        "plt.savefig(\"inference_visualization_original_nerf.png\")\n",
        "print(\"Inference visualization saved as inference_visualization_original_nerf.png\")\n",
        "plt.close(fig)\n",
        "\n",
        "# Functions for novel view synthesis video (can be adapted from your original code)\n",
        "def get_translation_t(t):\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "def get_rotation_phi(phi): # around X-axis\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "def get_rotation_theta(theta): # around Y-axis\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = get_translation_t(radius)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    # This is a common transformation to align camera axes with standard conventions\n",
        "    c2w = tf.convert_to_tensor(np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]), dtype=tf.float32) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "print(\"Generating novel view synthesis video...\")\n",
        "rgb_frames = []\n",
        "num_video_frames = 60 # Fewer frames for quicker generation for testing\n",
        "video_batch_size = BATCH_SIZE # Process in batches if memory is an issue\n",
        "\n",
        "for i in tqdm(range(0, num_video_frames, video_batch_size)):\n",
        "    current_batch_size = min(video_batch_size, num_video_frames - i)\n",
        "    batch_rays_o = []\n",
        "    batch_rays_d = []\n",
        "    batch_t_vals_coarse = []\n",
        "\n",
        "    for j in range(current_batch_size):\n",
        "        theta = (i + j) * (360.0 / num_video_frames)\n",
        "        c2w = pose_spherical(theta, -30.0, 4.0) # Example view parameters\n",
        "\n",
        "        ray_origins_single, ray_directions_single = get_rays(H, W, focal, c2w)\n",
        "        # Expand dims to simulate batch for render_flat_rays if it expects it,\n",
        "        # or adjust render_flat_rays. Here, get_rays already returns unbatched HxWx3.\n",
        "        # We will batch them *after* this loop.\n",
        "\n",
        "        # Coarse t_vals for this single pose (needs to be shaped for the model later)\n",
        "        # Here, we ensure t_vals are generated for a single \"image\" view.\n",
        "        # The render_flat_rays will produce t_vals of shape (H, W, N_samples)\n",
        "        # We need to add a batch dim for the model.\n",
        "        _, _, t_vals_coarse_single = render_flat_rays(\n",
        "            ray_origins_single[None,...], ray_directions_single[None,...], # Add batch dim for this call\n",
        "            near=2.0, far=6.0,\n",
        "            num_samples=NUM_SAMPLES_COARSE,\n",
        "            rand=False # No random sampling for video generation\n",
        "        )\n",
        "        # t_vals_coarse_single is (1, H, W, N_samples), remove batch for list append\n",
        "        batch_rays_o.append(ray_origins_single)\n",
        "        batch_rays_d.append(ray_directions_single)\n",
        "        batch_t_vals_coarse.append(tf.squeeze(t_vals_coarse_single, axis=0))\n",
        "\n",
        "\n",
        "    # Stack to create a batch\n",
        "    rays_o_batch_video = tf.stack(batch_rays_o, axis=0) # (current_batch_size, H, W, 3)\n",
        "    rays_d_batch_video = tf.stack(batch_rays_d, axis=0) # (current_batch_size, H, W, 3)\n",
        "    t_vals_coarse_batch_video = tf.stack(batch_t_vals_coarse, axis=0) # (current_batch_size, H, W, N_coarse)\n",
        "\n",
        "    # Infer with the model\n",
        "    rgb_fine_video_batch, _ = nerf_system(\n",
        "        (rays_o_batch_video, rays_d_batch_video, t_vals_coarse_batch_video), training=False\n",
        "    )\n",
        "\n",
        "    # Process and store frames\n",
        "    for k in range(current_batch_size):\n",
        "        img_np = np.clip(255 * rgb_fine_video_batch[k].numpy(), 0.0, 255.0).astype(np.uint8)\n",
        "        rgb_frames.append(img_np)\n",
        "\n",
        "\n",
        "if rgb_frames:\n",
        "    rgb_video_path = \"rgb_video_original_nerf.mp4\"\n",
        "    imageio.mimwrite(rgb_video_path, rgb_frames, fps=30, quality=8, macro_block_size=1) # quality 1-10\n",
        "    print(f\"Novel view video saved as {rgb_video_path}\")\n",
        "else:\n",
        "    print(\"No frames generated for the video.\")"
      ],
      "metadata": {
        "id": "oVv1ovzx586H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nerf",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}